# Neural Network Optimizers Comparison ğŸ§ 

Una implementaciÃ³n desde cero en Rust que compara tres algoritmos de optimizaciÃ³n populares para redes neuronales: **Momentum**, **RMSprop** y **Adam**.

## ğŸ¯ DescripciÃ³n

Este proyecto implementa una red neuronal feedforward simple y entrena tres copias idÃ©nticas usando diferentes optimizadores en hilos paralelos. El objetivo es comparar su rendimiento al resolver el problema clÃ¡sico de la funciÃ³n XOR.

### Â¿Por quÃ© XOR?

La funciÃ³n XOR es un problema no linealmente separable que requiere al menos una capa oculta para ser resuelto. Es un benchmark clÃ¡sico para evaluar algoritmos de aprendizaje de redes neuronales.

| Entrada A | Entrada B | Salida |
|-----------|-----------|--------|
| 0         | 0         | 0      |
| 0         | 1         | 1      |
| 1         | 0         | 1      |
| 1         | 1         | 0      |

## ğŸ—ï¸ Arquitectura de la Red

- **Capa de entrada**: 2 neuronas
- **Capa oculta**: 4 neuronas (funciÃ³n de activaciÃ³n: sigmoid)
- **Capa de salida**: 1 neurona (funciÃ³n de activaciÃ³n: sigmoid)
- **InicializaciÃ³n**: Pesos aleatorios con semilla fija para reproducibilidad

## ğŸš€ Algoritmos de OptimizaciÃ³n

### 1. Momentum Optimizer
```
velocidad = Î² Ã— velocidad_anterior + gradiente_actual
peso = peso - learning_rate Ã— velocidad
```
- **Î²**: 0.9 (factor de momentum)
- **Ventaja**: Suaviza oscilaciones, ayuda a escapar mÃ­nimos locales
- **Desventaja**: Puede oscilar en valles estrechos

### 2. RMSprop Optimizer
```
cache = Î± Ã— cache_anterior + (1-Î±) Ã— gradienteÂ²
peso = peso - learning_rate Ã— gradiente / âˆš(cache + Îµ)
```
- **Î±**: 0.9 (factor de decaimiento)
- **Îµ**: 1e-8 (para estabilidad numÃ©rica)
- **Ventaja**: Se adapta automÃ¡ticamente a diferentes escalas de gradientes
- **Desventaja**: Puede convergir prematuramente en algunos casos

### 3. Adam Optimizer (Adaptive Moment Estimation)
```
m = Î²â‚ Ã— m_anterior + (1-Î²â‚) Ã— gradiente          // momentum
v = Î²â‚‚ Ã— v_anterior + (1-Î²â‚‚) Ã— gradienteÂ²         // varianza
m_corregido = m / (1 - Î²â‚^t)                      // correcciÃ³n de sesgo
v_corregido = v / (1 - Î²â‚‚^t)
peso = peso - learning_rate Ã— m_corregido / (âˆšv_corregido + Îµ)
```
- **Î²â‚**: 0.9 (factor de momentum)
- **Î²â‚‚**: 0.999 (factor de decaimiento para la varianza)
- **Îµ**: 1e-8 (para estabilidad numÃ©rica)
- **Ventaja**: Combina lo mejor de Momentum y RMSprop
- **Desventaja**: MÃ¡s complejo computacionalmente

## ğŸ› ï¸ CaracterÃ­sticas TÃ©cnicas

### Concurrencia
- Entrenamiento en **3 hilos paralelos** simultÃ¡neos
- ComunicaciÃ³n entre hilos usando `std::sync::mpsc` channels
- SincronizaciÃ³n de resultados con `Arc<Mutex<Vec<T>>>`

### ImplementaciÃ³n
- **Generador de nÃºmeros aleatorios personalizado**: Linear Congruential Generator (LCG)
- **Sin dependencias externas**: ImplementaciÃ³n completamente desde cero
- **Trait-based polymorphism**: Interfaz comÃºn para todos los optimizadores
- **Memory safety**: Aprovecha las garantÃ­as de seguridad de memoria de Rust

## ğŸ“Š Salida del Programa

```
ğŸ§  COMPARACIÃ“N DE OPTIMIZADORES: Momentum vs RMSprop vs Adam
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Datos de entrenamiento (funciÃ³n XOR):
   [0.0, 0.0] â†’ [0.0]
   [0.0, 1.0] â†’ [1.0]
   [1.0, 0.0] â†’ [1.0]
   [1.0, 1.0] â†’ [0.0]

ğŸš€ Iniciando entrenamiento en paralelo...
   Learning rate: 0.1
   Ã‰pocas mÃ¡ximas: 10000
   Error objetivo: 0.01

ADAM: Ã‰poca 0, Error = 0.575620
MOMENTUM: Ã‰poca 0, Error = 0.508189
RMSPROP: Ã‰poca 0, Error = 0.742123
ğŸ‰ RMSPROP CONVERGIÃ“ en Ã©poca 80 con error 0.009616!
ğŸ‰ ADAM CONVERGIÃ“ en Ã©poca 140 con error 0.009810!
MOMENTUM: Ã‰poca 500, Error = 0.488309
MOMENTUM: Ã‰poca 1000, Error = 0.014858
ğŸ‰ MOMENTUM CONVERGIÃ“ en Ã©poca 1094 con error 0.009986!
...

ğŸ“ˆ RESULTADOS FINALES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¹ RMSPROP â†’ Error final: 0.009616 | Ã‰pocas: 81 | Tiempo: 3ms âœ…
ğŸ”¹ ADAM â†’ Error final: 0.009810 | Ã‰pocas: 141 | Tiempo: 6ms âœ…
ğŸ”¹ MOMENTUM â†’ Error final: 0.009986 | Ã‰pocas: 1095 | Tiempo: 26ms âœ…

ğŸ† GANADOR: RMSPROP con error final 0.009616 en 81 Ã©pocas (3ms)
```



## ğŸ“ˆ InterpretaciÃ³n de Resultados

### MÃ©tricas Evaluadas
- **Error final**: Menor es mejor
- **Ã‰pocas hasta convergencia**: Menor indica convergencia mÃ¡s rÃ¡pida
- **Tiempo de ejecuciÃ³n**: Menor es mÃ¡s eficiente
- **Convergencia**: Si alcanzÃ³ el error objetivo (< 0.01)

### Resultados TÃ­picos
En general, sueles observar estos patrones:

1. **Adam**: Convergencia mÃ¡s rÃ¡pida y estable
2. **RMSprop**: Buen balance entre velocidad y estabilidad
3. **Momentum**: Puede ser mÃ¡s lento pero a veces encuentra mejores mÃ­nimos

> **Nota**: Los resultados pueden variar segÃºn la semilla aleatoria y los hiperparÃ¡metros.

## ğŸ”§ PersonalizaciÃ³n

### Modificar HiperparÃ¡metros
En la funciÃ³n `main()`:
```rust
let learning_rate = 0.1;        // Tasa de aprendizaje
let max_epochs = 10000;         // Ã‰pocas mÃ¡ximas
let target_error = 0.01;        // Error objetivo
let seed = 42;                  // Semilla para reproducibilidad
```

### Modificar Arquitectura de Red
En `NeuralNetwork::new_with_seed()`:
```rust
let network = NeuralNetwork::new_with_seed(
    2,              // Neuronas de entrada
    4,              // Neuronas ocultas
    1,              // Neuronas de salida
    learning_rate,
    seed
);
```

### Modificar ParÃ¡metros de Optimizadores
```rust
// Momentum
let optimizer = MomentumOptimizer::new(&network, 0.9);  // Î²

// RMSprop  
let optimizer = RMSpropOptimizer::new(&network, 0.9, 1e-8);  // Î±, Îµ

// Adam
let optimizer = AdamOptimizer::new(&network, 0.9, 0.999, 1e-8);  // Î²â‚, Î²â‚‚, Îµ
```

## ğŸ“š Conceptos Aprendidos

Este proyecto ilustra varios conceptos importantes:

- **Backpropagation**: CÃ¡lculo de gradientes mediante la regla de la cadena
- **Algoritmos de optimizaciÃ³n**: Diferentes estrategias para actualizar pesos
- **Paralelismo en Rust**: Uso seguro de hilos y sincronizaciÃ³n
- **Trait system**: Polimorfismo sin overhead de runtime
- **Memory management**: Ownership, borrowing y lifetimes



---

**Â¿Encontraste este proyecto Ãºtil?** â­ Â¡Dale una estrella al repositorio!