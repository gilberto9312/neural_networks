# Redes Neuronales en Rust: Un Reto de 21 D√≠as

Este repositorio documenta un reto de 21 d√≠as para aprender los conceptos fundamentales de las redes neuronales desde cero, utilizando Rust. Puedes seguir el reto d√≠a a d√≠a o consultar la gu√≠a tem√°tica para encontrar conceptos espec√≠ficos.

## üöÄ C√≥mo Empezar

Cada proyecto es una aplicaci√≥n de Rust independiente. Para ejecutar cualquiera de ellos, sigue estos pasos:

1.  Navega al directorio del proyecto que te interese.
2.  Ejecuta el proyecto usando Cargo.

```bash
# Ejemplo para ejecutar el proyecto del D√≠a 5: Perceptr√≥n
cd perceptron
cargo run
```

---

## üóìÔ∏è Cronolog√≠a del Reto (D√≠a a D√≠a)

Esta es la progresi√≥n recomendada para seguir el reto de forma secuencial.

| D√≠a | Proyecto | Concepto Clave |
| :-- | :--- | :--- |
| 1 | [`neurons`](./neurons/) | Neurona Artificial |
| 2 | [`sigmoid`](./sigmoid/) | Funci√≥n de Activaci√≥n Sigmoid |
| 3 | [`tanh`](./tanh/) | Funci√≥n de Activaci√≥n Tanh |
| 4 | [`relu`](./relu/) | Funci√≥n de Activaci√≥n ReLU |
| 5 | [`perceptron`](./perceptron/) | Perceptr√≥n Simple y Aprendizaje |
| 6 | [`xor`](./xor/) | Red Neuronal Multicapa |
| 7 | [`momentum`](./momentum/) | Optimizador Momentum |
| 8 | [`comparation-optimizer`](./comparation-optimizer/) | Comparaci√≥n de Optimizadores (Momentum, RMSprop, Adam) |
| 9 | [`regularization`](./regularization/) | Regularizaci√≥n (L1, L2, Dropout) |
| 10 | [`graph`](./graph/) | Visualizaci√≥n de M√©tricas de Entrenamiento |
| 11 | [`earlyStopping`](./earlyStopping/) | Parada Temprana (Early Stopping) |
| 12 | [`dataset_iris`](./dataset_iris/) | Aplicaci√≥n Pr√°ctica: Clasificaci√≥n Multiclase |
| 13 | [`dataset_mnist`](./dataset_mnist/) | Aplicaci√≥n Pr√°ctica: Clasificaci√≥n png  |

---

## üìö Gu√≠a Tem√°tica de Conceptos

Usa este √≠ndice para encontrar proyectos relacionados con un tema espec√≠fico.

### 1. Fundamentos de Redes Neuronales
*   **[`neurons`](./neurons/)**: La unidad b√°sica de una red neuronal.
*   **[`perceptron`](./perceptron/)**: Un modelo de una sola neurona que puede aprender problemas linealmente separables.
*   **[`xor`](./xor/)**: La necesidad de capas ocultas para resolver problemas no lineales.

### 2. Funciones de Activaci√≥n
*   **[`sigmoid`](./sigmoid/)**: Transforma salidas a un rango de probabilidad (0, 1).
*   **[`tanh`](./tanh/)**: Una alternativa a Sigmoid centrada en cero (-1, 1).
*   **[`relu`](./relu/)**: La funci√≥n de activaci√≥n m√°s popular por su eficiencia y simplicidad.

### 3. Optimizaci√≥n del Entrenamiento
*   **[`momentum`](./momentum/)**: Acelera el entrenamiento a√±adiendo "inercia" a la actualizaci√≥n de pesos.
*   **[`comparation-optimizer`](./comparation-optimizer/)**: Compara el rendimiento de Momentum, RMSprop y Adam.

### 4. T√©cnicas de Regularizaci√≥n (Prevenci√≥n de Overfitting)
*   **[`regularization`](./regularization/)**: Implementa L1, L2 y Dropout para mejorar la generalizaci√≥n del modelo.
*   **[`earlyStopping`](./earlyStopping/)**: Detiene el entrenamiento de forma inteligente para evitar el sobreajuste.

### 5. An√°lisis y Aplicaciones Pr√°cticas
*   **[`graph`](./graph/)**: Visualiza las curvas de error para diagnosticar el entrenamiento.
*   **[`dataset_iris`](./dataset_iris/)**: Resuelve un problema de clasificaci√≥n del mundo real de principio a fin.
*   **[`dataset_mnist`](./dataset_mnist/)**: Resuelve un problema de clasificaci√≥n de imagenes.