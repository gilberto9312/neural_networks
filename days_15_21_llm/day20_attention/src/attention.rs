// Scaled Dot-Product Attention

// TODO: Implementar atención básica
// - Attention(Q,K,V) = softmax(QK^T / √d_k)V
// - Máscaras de atención
