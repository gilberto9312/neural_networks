# ğŸ§  Redes Neuronales - Serie de Aprendizaje Diario

## ğŸ“… DÃ­a 15: Modelos N-gram

### ğŸ¯ Objetivo del DÃ­a
Entender cÃ³mo funcionan los modelos de lenguaje estadÃ­sticos mÃ¡s simples (N-grams), aprender a calcular probabilidades a partir de frecuencias en un corpus, y usar estos modelos para **generar texto automÃ¡ticamente**.

Este es el primer paso antes de llegar a los Transformers modernos. Si no entiendes N-grams, no entenderÃ¡s quÃ© problemas resuelven los modelos neuronales.

---

## ğŸ” Â¿QuÃ© es un Modelo N-gram?

Un modelo N-gram es un modelo de lenguaje que **predice la siguiente palabra** basÃ¡ndose en las **N-1 palabras anteriores**.

### Los tres tipos que implementaremos:

#### 1ï¸âƒ£ **Unigram** (N=1)
Ignora el contexto completamente. Solo mira quÃ© palabras son mÃ¡s frecuentes en el corpus.

```
P(palabra) = count(palabra) / total_palabras
```

**Ejemplo**: Si "the" aparece 1000 veces en un corpus de 10,000 palabras:
```
P("the") = 1000/10,000 = 0.1 = 10%
```

#### 2ï¸âƒ£ **Bigram** (N=2)
Usa **1 palabra** de contexto para predecir la siguiente.

```
P(w2 | w1) = count(w1, w2) / count(w1)
```

**Ejemplo**: Si "the music" aparece 50 veces y "the" aparece 1000 veces:
```
P("music" | "the") = 50/1000 = 0.05 = 5%
```

#### 3ï¸âƒ£ **Trigram** (N=3)
Usa **2 palabras** de contexto para predecir la siguiente.

```
P(w3 | w1, w2) = count(w1, w2, w3) / count(w1, w2)
```

**Ejemplo**: Si "in the club" aparece 10 veces y "in the" aparece 100 veces:
```
P("club" | "in the") = 10/100 = 0.1 = 10%
```

---

## â“ Â¿Por quÃ© Necesitamos Modelos de Lenguaje?

Imagina que estÃ¡s escribiendo un mensaje y tu teclado quiere **autocompletar** la siguiente palabra. Â¿CÃ³mo sabe quÃ© sugerir?

```
Usuario escribe: "I went to the"
Opciones posibles: "store", "beach", "moon", "elephant"
```

Un modelo de lenguaje asigna **probabilidades** a cada palabra:
- P("store" | "I went to the") = 0.25 â† Muy probable âœ…
- P("beach" | "I went to the") = 0.20 â† Probable âœ…
- P("moon" | "I went to the") = 0.05 â† Poco probable ğŸ¤”
- P("elephant" | "I went to the") = 0.01 â† Muy raro âŒ

El modelo **muestrea** de esta distribuciÃ³n para elegir la siguiente palabra.

---

## ğŸ—ï¸ Estructura del CÃ³digo

```
day15_ngram_models/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs          # Demo completa con los 3 modelos
â”‚   â”œâ”€â”€ dataset.rs       # Carga Africa Galore + tokenizaciÃ³n
â”‚   â”œâ”€â”€ ngram.rs         # UnigramModel, BigramModel, TrigramModel
â”‚   â””â”€â”€ sampling.rs      # GeneraciÃ³n de texto con muestreo
â”œâ”€â”€ Cargo.toml
â””â”€â”€ README.md
```

### Â¿QuÃ© hace cada mÃ³dulo?

#### `dataset.rs` - Preprocesamiento
```rust
// Carga el JSON del dataset
let texts = load_africa_galore("../../datasets/africa_galore.json")?;

// Tokeniza: "Hello, world!" â†’ ["hello", "world"]
let tokens = tokenize("Hello, world!");
// Resultado: ["hello", "world"]
```

**Â¿Por quÃ© convertir a minÃºsculas?** Para que "The" y "the" se cuenten como la misma palabra.

#### `ngram.rs` - Los Modelos
```rust
// Entrena un modelo bigram
let bigram_model = BigramModel::new(&tokens);

// Calcula probabilidad condicional
let prob = bigram_model.probability("the", "music");
// P("music" | "the") = ?
```

#### `sampling.rs` - GeneraciÃ³n de Texto
```rust
// Genera 30 palabras empezando con "the"
let text = generate_bigram(&model, "the", 30);
// Resultado: "the music was playing in the club and people were dancing..."
```

**Â¿Por quÃ© muestreo aleatorio?** Si siempre elegimos la palabra mÃ¡s probable, el texto serÃ­a muy repetitivo y aburrido.

---

## ğŸ§ª Experimentos que Realizamos

### 1. TokenizaciÃ³n del Corpus
Tomamos el dataset **Africa Galore** (232 pÃ¡rrafos sobre cultura africana) y lo dividimos en tokens:

```
Texto original:
"The Lagos air was thick with humidity, but the energy in the club was electric."

Tokens generados:
["the", "lagos", "air", "was", "thick", "with", "humidity", "but", "the", "energy", ...]
```

**Total**: ~31,000 tokens
**Vocabulario Ãºnico**: ~5,100 palabras

### 2. DivisiÃ³n Train/Test (80/20)
```
Entrenamiento: 24,800 tokens â†’ Para calcular frecuencias
Prueba:        6,200 tokens  â†’ Para evaluar perplexity
```

**Â¿Por quÃ© dividir?** Para asegurarnos de que el modelo funciona con texto que **nunca ha visto**.

### 3. GeneraciÃ³n de Texto

#### Unigram (sin contexto)
```
Prompt: "Jide was hungry so"
GeneraciÃ³n: "the music a in was people of and traditional..."
```
âŒ **No tiene sentido** - solo elige palabras frecuentes al azar.

#### Bigram (1 palabra de contexto)
```
Prompt: "Jide was hungry so"
GeneraciÃ³n: "she went looking for food in the market to buy..."
```
âœ… **Algo mejor** - las palabras tienen mÃ¡s coherencia local.

#### Trigram (2 palabras de contexto)
```
Prompt: "Jide was hungry so"
GeneraciÃ³n: "she went looking for a traditional dish made with..."
```
âœ…âœ… **Mucho mejor** - frases mÃ¡s coherentes y gramaticales.

---

## ğŸ“Š Â¿CÃ³mo Medimos si un Modelo es Bueno?

Usamos una mÃ©trica llamada **Perplexity**.

### Â¿QuÃ© es Perplexity?

Es una medida de **quÃ© tan sorprendido estÃ¡ el modelo** ante nuevas palabras.

```
Perplexity = exp(-1/N * Î£ log P(palabra_i | contexto))
```

**InterpretaciÃ³n**:
- Perplexity de 100 = El modelo estÃ¡ tan confundido como si tuviera que elegir entre **100 palabras al azar**
- **Menor perplexity = mejor modelo**

### Resultados en Africa Galore:
```
Unigram:   Perplexity = 342  â† Muy confundido
Bigram:    Perplexity = 128  â† Mejor
Trigram:   Perplexity =  68  â† Â¡Mucho mejor!
```

**ConclusiÃ³n**: MÃ¡s contexto = mejores predicciones

---

## âš ï¸ El Gran Problema: Data Sparsity

AquÃ­ viene el **problema masivo** de los N-grams.

### Â¿QuÃ© es Data Sparsity?

A medida que aumentas N, la cantidad de **combinaciones posibles** explota:

```
Vocabulario: 5,100 palabras

Bigramas posibles:  5,100 Ã— 5,100 = 26 millones
Trigramas posibles: 5,100Â³ = 132,000 millones
```

**Pero en nuestro dataset solo tenemos ~31,000 tokens.**

Esto significa que la **mayorÃ­a de combinaciones NUNCA aparecen**:

```
Bigrams con count = 0:  99.95% ğŸ˜±
Trigrams con count = 0: 99.98% ğŸ˜±ğŸ˜±
```

### Â¿QuÃ© pasa cuando el modelo ve una secuencia nueva?

```python
# Bigram que nunca apareciÃ³ en el dataset
model.probability("purple", "elephant")
# â†’ 0.0 (no puede predecir nada)
```

El modelo **se queda atascado** y no puede generar mÃ¡s texto.

**SoluciÃ³n temporal**: Asignar una probabilidad muy pequeÃ±a (1e-10) en lugar de 0.

**SoluciÃ³n real**: Usar modelos neuronales (Transformers) que **generalizan** mejor.

---

## ğŸ’¡ Conceptos Clave Aprendidos

### 1. Probabilidad Condicional
Los modelos de lenguaje funcionan calculando **P(siguiente_palabra | contexto)**.

### 2. Trade-off Contexto vs Datos
- **MÃ¡s contexto** (trigram) = mejores predicciones
- **Pero** requiere **muchos mÃ¡s datos** para entrenar bien

### 3. Muestreo EstocÃ¡stico
No siempre elegimos la palabra con mayor probabilidad. Usamos `WeightedIndex` de Rust para muestrear segÃºn probabilidades:

```rust
let weights = [0.5, 0.3, 0.2];  // Probabilidades
let words = ["the", "a", "an"];
let chosen = sample_weighted(&words, &weights);
// 50% chance de "the", 30% de "a", 20% de "an"
```

Esto hace el texto **mÃ¡s creativo y menos repetitivo**.

### 4. TokenizaciÃ³n es Importante
```
Texto mal tokenizado: ["Hello", "world", "!"]
Texto bien tokenizado: ["hello", "world"]
```

Una mala tokenizaciÃ³n puede arruinar todo el modelo.

### 5. LÃ­mites de N-grams
- Solo ven **N-1 palabras de contexto**
- No entienden **significado** (no saben que "dog" y "puppy" son similares)
- Sufren de **data sparsity** severa
- No pueden usar contexto de hace 50 palabras

**Por eso existen los Transformers** (DÃ­a 21) que resuelven todos estos problemas.

---

## ğŸ”§ CÃ³mo Ejecutar

```bash
# Navegar al proyecto
cd days_15_21_llm/day15_ngram_models

# Compilar y ejecutar
cargo run --release

# Ejecutar tests
cargo test

# Solo compilar
cargo build --release
```

---

## ğŸ“ˆ Salida Esperada

```
ğŸš€ Modelos N-gram - DÃ­a 15
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¦ Cargando dataset Africa Galore...
âœ… Dataset cargado: 232 textos

ğŸ”¤ Tokenizando corpus...
âœ… Total de tokens: 31,234
âœ… Vocabulario Ãºnico: 5,143 palabras
âœ… Tokens de entrenamiento: 24,987
âœ… Tokens de prueba: 6,247

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š MODELO UNIGRAM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Entrenando modelo Unigram...
âœ… Modelo Unigram entrenado

ğŸ“ˆ Top 10 palabras mÃ¡s frecuentes:
   1. 'the' - 1234 veces (P=0.0495)
   2. 'of' - 567 veces (P=0.0227)
   3. 'and' - 456 veces (P=0.0183)
   ...

âœï¸  GeneraciÃ³n de texto (Unigram - 30 palabras):
   the music a in of was people and tradition with culture to...

ğŸ“‰ Perplexity (Unigram): 342.15

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š MODELO BIGRAM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Entrenando modelo Bigram...
âœ… Modelo Bigram entrenado

ğŸ” Ejemplos de probabilidades P(w2|w1):
   P('music' | 'the') = 0.0245
   P('the' | 'in') = 0.1234
   P('a' | 'was') = 0.0567

âœï¸  GeneraciÃ³n de texto (Bigram - 30 palabras):
   the music was playing in the club and people were dancing to...

ğŸ“‰ Perplexity (Bigram): 127.83

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š MODELO TRIGRAM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Entrenando modelo Trigram...
âœ… Modelo Trigram entrenado

ğŸ” Ejemplos de probabilidades P(w3|w1,w2):
   P('was' | 'the', 'music') = 0.3333
   P('club' | 'in', 'the') = 0.0833
   P('music' | 'of', 'the') = 0.0456

âœï¸  GeneraciÃ³n de texto (Trigram - 30 palabras):
   the music was a celebration of life and culture in the heart of africa...

ğŸ“‰ Perplexity (Trigram): 68.42

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ† COMPARACIÃ“N DE MODELOS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Resumen de Perplexity (menor es mejor):
   Unigram:  342.15
   Bigram:   127.83
   Trigram:   68.42

ğŸ’¡ InterpretaciÃ³n:
   - Perplexity mide quÃ© tan 'sorprendido' estÃ¡ el modelo
   - Menor perplexity = mejor predicciÃ³n
   - Modelos de mayor orden (trigram) suelen tener menor perplexity
   - Pero requieren mÃ¡s datos y pueden sufrir de overfitting

âœ… AnÃ¡lisis completo de modelos N-gram finalizado!
```

---

## âš™ï¸ ParÃ¡metros que Puedes Ajustar

### En `main.rs`:

```rust
// NÃºmero de palabras a generar
let num_words = 50;  // Prueba con 10, 30, 100

// Palabra inicial para bigram
let start_word = "music";  // Prueba: "africa", "the", "celebration"

// Par inicial para trigram
let start_pair = ("the", "music");  // Prueba diferentes pares
```

### DivisiÃ³n train/test:
```rust
// Cambiar el ratio de divisiÃ³n (actualmente 80/20)
let split_index = (all_tokens.len() as f64 * 0.9) as usize;  // 90/10
```

---

## ğŸ› Â¿QuÃ© puede salir mal?

### 1. El modelo no puede continuar
```
âš ï¸ No valid continuation found.
```

**Causa**: El bigram/trigram nunca apareciÃ³ en el dataset.
**SoluciÃ³n**: Usa un prompt que aparezca en el corpus, o usa un modelo de menor orden (unigram siempre funciona).

### 2. Texto generado no tiene sentido
**Causa Normal**: Los N-grams son modelos muy simples.
**Mejora**: Usa trigram en vez de unigram, aumenta el tamaÃ±o del dataset.

### 3. Dataset no encontrado
```
âŒ Error cargando dataset
```

**SoluciÃ³n**: AsegÃºrate de ejecutar desde `days_15_21_llm/day15_ngram_models/` y que `datasets/africa_galore.json` existe.

---

## ğŸ¯ PrÃ³ximos Pasos

Este es solo el comienzo. En los siguientes dÃ­as:

- **DÃ­a 16**: AnÃ¡lisis avanzado de perplexity
- **DÃ­a 17**: TokenizaciÃ³n BPE (mucho mejor que space tokenizer)
- **DÃ­a 18**: Embeddings (Word2Vec - entiende similitudes semÃ¡nticas)
- **DÃ­a 19**: MLP para clasificaciÃ³n de texto
- **DÃ­a 20**: Mecanismo de atenciÃ³n (el corazÃ³n de los Transformers)
- **DÃ­a 21**: **Transformer completo** - Un LLM real desde cero

---

## ğŸ“š Dependencias

```toml
[dependencies]
rand = "0.8"           # Para muestreo de distribuciones
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"     # Para cargar JSON
```

**Sin dependencias de ML pesadas** - Todo implementado desde cero para aprender.

---

## ğŸ“– Referencias

- [N-gram Language Models - Stanford](https://web.stanford.edu/~jurafsky/slp3/)
- [Perplexity Explained](https://en.wikipedia.org/wiki/Perplexity)
- [Africa Galore Dataset](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json)

---

*Implementado en Rust como parte del desafÃ­o de 21 dÃ­as de Neural Networks desde cero.*
