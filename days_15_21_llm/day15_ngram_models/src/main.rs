// Modelos N-gram desde Cero
// DÃ­a 15-16: Fundamentos de Modelos de Lenguaje

mod dataset;
mod ngram;
mod sampling;

use dataset::{load_africa_galore, tokenize_corpus};
use ngram::{
    calculate_perplexity_bigram, calculate_perplexity_trigram, calculate_perplexity_unigram,
    BigramModel, TrigramModel, UnigramModel,
};
use rand::thread_rng;
use sampling::{generate_bigram, generate_trigram, generate_unigram, random_start_bigram};

fn main() {
    println!("ğŸš€ Modelos N-gram - DÃ­a 15");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();

    // 1. Cargar dataset Africa Galore
    println!("ğŸ“¦ Cargando dataset Africa Galore...");
    let dataset_path = "../../datasets/africa_galore.json";
    let texts = match load_africa_galore(dataset_path) {
        Ok(texts) => {
            println!("âœ… Dataset cargado: {} textos", texts.len());
            texts
        }
        Err(e) => {
            eprintln!("âŒ Error cargando dataset: {}", e);
            eprintln!("ğŸ’¡ AsegÃºrate de ejecutar desde: days_15_21_llm/day15_ngram_models/");
            eprintln!("ğŸ’¡ Y que el dataset estÃ© en: datasets/africa_galore.json");
            return;
        }
    };

    // 2. Tokenizar corpus
    println!("\nğŸ”¤ Tokenizando corpus...");
    let all_tokens = tokenize_corpus(&texts);
    println!("âœ… Total de tokens: {}", all_tokens.len());

    // Calcular vocabulario Ãºnico
    let vocab_size: std::collections::HashSet<_> = all_tokens.iter().collect();
    println!("âœ… Vocabulario Ãºnico: {} palabras", vocab_size.len());

    // Dividir en entrenamiento (80%) y prueba (20%)
    let split_index = (all_tokens.len() as f64 * 0.8) as usize;
    let train_tokens = &all_tokens[..split_index];
    let test_tokens = &all_tokens[split_index..];
    println!("âœ… Tokens de entrenamiento: {}", train_tokens.len());
    println!("âœ… Tokens de prueba: {}", test_tokens.len());

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ“Š MODELO UNIGRAM");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // 3. Entrenar modelo Unigram
    println!("\nğŸ“ Entrenando modelo Unigram...");
    let unigram_model = UnigramModel::new(train_tokens);
    println!("âœ… Modelo Unigram entrenado");

    // Mostrar palabras mÃ¡s frecuentes
    let mut word_freq: Vec<_> = unigram_model.word_frequencies().iter().collect();
    word_freq.sort_by(|a, b| b.1.cmp(a.1));
    println!("\nğŸ“ˆ Top 10 palabras mÃ¡s frecuentes:");
    for (i, (word, count)) in word_freq.iter().take(10).enumerate() {
        let prob = unigram_model.probability(word);
        println!("   {}. '{}' - {} veces (P={:.4})", i + 1, word, count, prob);
    }

    // Generar texto con Unigram
    println!("\nâœï¸  GeneraciÃ³n de texto (Unigram - 30 palabras):");
    let unigram_text = generate_unigram(&unigram_model, 30);
    println!("   {}", unigram_text.join(" "));

    // Calcular perplexity
    let unigram_perplexity = calculate_perplexity_unigram(&unigram_model, test_tokens);
    println!("\nğŸ“‰ Perplexity (Unigram): {:.2}", unigram_perplexity);

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ“Š MODELO BIGRAM");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // 4. Entrenar modelo Bigram
    println!("\nğŸ“ Entrenando modelo Bigram...");
    let bigram_model = BigramModel::new(train_tokens);
    println!("âœ… Modelo Bigram entrenado");

    // Ejemplo de probabilidades condicionales
    println!("\nğŸ” Ejemplos de probabilidades P(w2|w1):");
    let example_pairs = [("the", "music"), ("in", "the"), ("was", "a")];
    for (w1, w2) in &example_pairs {
        let prob = bigram_model.probability(w1, w2);
        println!("   P('{}' | '{}') = {:.4}", w2, w1, prob);
    }

    // Generar texto con Bigram
    println!("\nâœï¸  GeneraciÃ³n de texto (Bigram - 30 palabras):");
    let bigram_text = generate_bigram(&bigram_model, "the", 29);
    println!("   {}", bigram_text.join(" "));

    // Calcular perplexity
    let bigram_perplexity = calculate_perplexity_bigram(&bigram_model, test_tokens);
    println!("\nğŸ“‰ Perplexity (Bigram): {:.2}", bigram_perplexity);

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ“Š MODELO TRIGRAM");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // 5. Entrenar modelo Trigram
    println!("\nğŸ“ Entrenando modelo Trigram...");
    let trigram_model = TrigramModel::new(train_tokens);
    println!("âœ… Modelo Trigram entrenado");

    // Ejemplo de probabilidades condicionales
    println!("\nğŸ” Ejemplos de probabilidades P(w3|w1,w2):");
    let example_triples = [("the", "music", "was"), ("in", "the", "club"), ("of", "the", "music")];
    for (w1, w2, w3) in &example_triples {
        let prob = trigram_model.probability(w1, w2, w3);
        println!("   P('{}' | '{}', '{}') = {:.4}", w3, w1, w2, prob);
    }

    // Generar texto con Trigram
    println!("\nâœï¸  GeneraciÃ³n de texto (Trigram - 30 palabras):");
    let mut rng = thread_rng();
    let vocab = bigram_model.vocabulary();
    let start_pair = random_start_bigram(&vocab, &mut rng);
    let trigram_text = generate_trigram(&trigram_model, (&start_pair.0, &start_pair.1), 28);
    println!("   {}", trigram_text.join(" "));

    // Calcular perplexity
    let trigram_perplexity = calculate_perplexity_trigram(&trigram_model, test_tokens);
    println!("\nğŸ“‰ Perplexity (Trigram): {:.2}", trigram_perplexity);

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("ğŸ† COMPARACIÃ“N DE MODELOS");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    println!("\nğŸ“Š Resumen de Perplexity (menor es mejor):");
    println!("   Unigram:  {:.2}", unigram_perplexity);
    println!("   Bigram:   {:.2}", bigram_perplexity);
    println!("   Trigram:  {:.2}", trigram_perplexity);

    println!("\nğŸ’¡ InterpretaciÃ³n:");
    println!("   - Perplexity mide quÃ© tan 'sorprendido' estÃ¡ el modelo");
    println!("   - Menor perplexity = mejor predicciÃ³n");
    println!("   - Modelos de mayor orden (trigram) suelen tener menor perplexity");
    println!("   - Pero requieren mÃ¡s datos y pueden sufrir de overfitting");

    println!("\nâœ… AnÃ¡lisis completo de modelos N-gram finalizado!");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
}
