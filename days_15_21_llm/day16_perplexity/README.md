# ğŸ§  Redes Neuronales - Serie de Aprendizaje LLM

## ğŸ“… DÃ­a 16: EvaluaciÃ³n con Perplexity

### ğŸ¯ Objetivo del DÃ­a
Comprender cÃ³mo evaluar modelos de lenguaje usando perplexity, comparar el rendimiento de diferentes modelos N-gram, y entender por quÃ© esta mÃ©trica es fundamental para medir la calidad de predicciÃ³n en modelos de lenguaje.

### ğŸ” Â¿QuÃ© es Perplexity?

Perplexity es una mÃ©trica que mide **quÃ© tan "sorprendido" estÃ¡ un modelo** ante datos nuevos. Es la forma estÃ¡ndar de evaluar modelos de lenguaje.

#### FÃ³rmula matemÃ¡tica:
```
Perplexity = exp(-1/N * Î£ log P(palabra_i | contexto))
```

Donde:
- `N` es el nÃºmero total de palabras
- `P(palabra_i | contexto)` es la probabilidad que el modelo asigna a cada palabra dado su contexto

#### Propiedades clave:
- **Menor perplexity = mejor modelo**: Un modelo con perplexity de 50 es mejor que uno con 100
- **InterpretaciÃ³n intuitiva**: Una perplexity de 100 significa que el modelo estÃ¡ tan confundido como si tuviera que elegir uniformemente entre 100 palabras
- **Sensibilidad al contexto**: Modelos de mayor orden (trigram) generalmente tienen menor perplexity que modelos simples (unigram)
- **Dependiente del dataset**: La perplexity solo es comparable entre modelos evaluados en el mismo conjunto de prueba

### ğŸ—ï¸ Estructura del CÃ³digo

Este proyecto extiende el trabajo del **DÃ­a 15** aÃ±adiendo anÃ¡lisis comparativo de perplexity.

#### MÃ³dulos utilizados (del DÃ­a 15):
1. **`ngram.rs`**: Modelos Unigram, Bigram, Trigram
2. **`dataset.rs`**: Carga de Africa Galore
3. **`sampling.rs`**: GeneraciÃ³n de texto

#### Funciones de evaluaciÃ³n:
- `calculate_perplexity_unigram()`: EvalÃºa modelo unigram
- `calculate_perplexity_bigram()`: EvalÃºa modelo bigram
- `calculate_perplexity_trigram()`: EvalÃºa modelo trigram

### ğŸ§ª Experimentos Realizados

#### 1. DivisiÃ³n Train/Test
- **80% entrenamiento**: Para estimar probabilidades
- **20% prueba**: Para evaluar perplexity sin sesgo

#### 2. ComparaciÃ³n de Modelos
Se comparan tres modelos entrenados en el mismo corpus:
- **Unigram**: Solo considera frecuencia de palabras individuales
- **Bigram**: Considera la palabra anterior
- **Trigram**: Considera las dos palabras anteriores

#### 3. AnÃ¡lisis de Sparsity
- Los modelos de mayor orden tienen mÃ¡s combinaciones posibles
- Muchas combinaciones nunca aparecen en el dataset (99.95%+ son ceros)
- Esto afecta la calidad de las predicciones

### ğŸ“Š Resultados TÃ­picos

BasÃ¡ndose en el dataset Africa Galore (232 pÃ¡rrafos):

```
Dataset: 232 textos, ~31,000 tokens
Vocabulario: ~5,100 palabras Ãºnicas

Resultados esperados:
- Unigram Perplexity:   ~250-400
- Bigram Perplexity:    ~80-150
- Trigram Perplexity:   ~50-100
```

**InterpretaciÃ³n**:
- El trigram tiene menor perplexity â†’ mejor predicciÃ³n
- Pero tambiÃ©n es mÃ¡s propenso a data sparsity
- Trade-off entre contexto y generalizaciÃ³n

### ğŸ’¡ Conceptos Clave Aprendidos

1. **MÃ©trica de evaluaciÃ³n objetiva**: Perplexity permite comparar modelos cuantitativamente, no solo cualitativamente

2. **Trade-off contexto vs. datos**: Modelos con mÃ¡s contexto (mayor N) pueden predecir mejor, pero requieren mucho mÃ¡s datos para entrenar bien

3. **Data sparsity**:
   - Bigramas: 5,143 Ã— 5,176 = 26M combinaciones posibles
   - Trigramas: 13,411 Ã— 5,142 = 68M combinaciones posibles
   - MÃ¡s del 99% nunca aparecen en el dataset

4. **Suavizado**: Cuando una secuencia no se ha visto, se asigna una probabilidad muy pequeÃ±a (1e-10) en lugar de 0 para evitar perplexity infinita

5. **ValidaciÃ³n cruzada**: Es fundamental evaluar en datos NO vistos durante el entrenamiento

6. **Limitaciones de N-grams**:
   - Contexto muy limitado (solo N-1 palabras)
   - No capturan similitudes semÃ¡nticas
   - ExplosiÃ³n combinatoria al aumentar N

### ğŸ”§ CÃ³mo Ejecutar

```bash
# Navegar al directorio
cd days_15_21_llm/day16_perplexity

# Compilar y ejecutar
cargo run --release

# Ver comparaciÃ³n detallada
cargo run --release -- --verbose
```

### ğŸ“ˆ Ejemplo de Salida

```
ğŸš€ EvaluaciÃ³n de Modelos N-gram - DÃ­a 16
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“¦ Dataset: 232 textos cargados
ğŸ“Š Tokens totales: 31,234
ğŸ“š Vocabulario: 5,143 palabras

ğŸ”€ DivisiÃ³n datos:
   Train: 24,987 tokens (80%)
   Test:  6,247 tokens (20%)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š RESULTADOS DE PERPLEXITY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Modelo      | Perplexity | Contexto
------------|------------|----------
Unigram     |   342.15   | Ninguno
Bigram      |   127.83   | 1 palabra
Trigram     |    68.42   | 2 palabras

âœ… Menor perplexity = Mejor modelo
ğŸ† Ganador: Trigram (68.42)

ğŸ’¡ El modelo trigram es 5x mejor que unigram
   en predecir la siguiente palabra.
```

### ğŸ“ Â¿Por quÃ© es Importante Perplexity?

1. **EvaluaciÃ³n estandarizada**: Permite comparar modelos diferentes de forma objetiva

2. **MÃ©trica interpretable**: A diferencia de otras mÃ©tricas complejas, perplexity tiene una interpretaciÃ³n intuitiva

3. **ConexiÃ³n con informaciÃ³n**: MatemÃ¡ticamente relacionada con la entropÃ­a de Shannon (teorÃ­a de informaciÃ³n)

4. **Predictor de calidad**: Correlaciona bien con la calidad percibida de generaciÃ³n de texto

5. **Base para modelos modernos**: Los transformers modernos tambiÃ©n se evalÃºan con perplexity

### ğŸ”¬ ComparaciÃ³n con Transformers

Mientras que los modelos N-gram del DÃ­a 15-16 tienen:
- Perplexity: 50-400 (segÃºn N)
- Contexto: 1-2 palabras

Los modelos Transformer modernos (que veremos en DÃ­a 21) logran:
- Perplexity: 10-30 en el mismo dataset
- Contexto: 512-8192 tokens
- Captura de relaciones semÃ¡nticas profundas

**Esto demuestra el poder de las arquitecturas neuronales modernas.**

### ğŸ“ Notas de Desarrollo

Este es el **DÃ­a 16** de una serie de aprendizaje progresivo sobre modelos de lenguaje. El cÃ³digo estÃ¡ diseÃ±ado para ser educativo, con:

- Comentarios detallados en espaÃ±ol
- CÃ¡lculos paso a paso de perplexity
- Comparaciones visuales claras
- Ejemplos con el dataset Africa Galore

**PrÃ³ximos pasos**:
- DÃ­a 17: TokenizaciÃ³n BPE (mejor que space tokenizer)
- DÃ­a 18: Embeddings (representaciones vectoriales)
- DÃ­a 19-21: Redes neuronales â†’ Transformer completo

### ğŸ”— ConexiÃ³n con el DÃ­a 15

Este proyecto **depende directamente** del DÃ­a 15:
- Usa los mismos modelos N-gram
- Trabaja con el mismo dataset
- Agrega la capa de evaluaciÃ³n cuantitativa

**Diferencia clave**: DÃ­a 15 se enfoca en *construir* modelos, DÃ­a 16 en *evaluarlos*.

### ğŸ“š Referencias TeÃ³ricas

- Perplexity es el exponencial de la entropÃ­a cruzada
- FormulaciÃ³n original en teorÃ­a de informaciÃ³n (Shannon, 1948)
- Ampliamente usada en papers de NLP desde los aÃ±os 90
- Sigue siendo mÃ©trica estÃ¡ndar para LLMs modernos

---

*Implementado en Rust para practicar tanto conceptos de ML como programaciÃ³n de sistemas.*
*Parte del plan maestro de aprendizaje de LLMs (dÃ­as 15-21).*
