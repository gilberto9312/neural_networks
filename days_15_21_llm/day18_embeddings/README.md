# ğŸ¯ Word Embeddings y Similitud SemÃ¡ntica

**DÃ­a 18 del DesafÃ­o de 21 DÃ­as - Embeddings y Representaciones**

## ğŸ“– DescripciÃ³n del Proyecto

Este proyecto implementa embeddings de palabras (word embeddings) utilizando una versiÃ³n simplificada del algoritmo **Skip-gram** de Word2Vec. Los embeddings son representaciones vectoriales densas de palabras que capturan relaciones semÃ¡nticas en un espacio de alta dimensiÃ³n.

La implementaciÃ³n estÃ¡ basada en el **Lab 2.5: Experiment with Embeddings** del curso AI Foundations de Google DeepMind, transpolado completamente a Rust.

## ğŸ“ Conceptos TeÃ³ricos

### Â¿QuÃ© son los Word Embeddings?

Los **word embeddings** son vectores densos de nÃºmeros reales que representan palabras en un espacio de alta dimensiÃ³n (tÃ­picamente 50-300 dimensiones). A diferencia de representaciones dispersas como one-hot encoding, los embeddings capturan similitudes semÃ¡nticas: palabras con significados similares tienen vectores cercanos en el espacio.

**Propiedades importantes:**
- Palabras similares â†’ vectores cercanos
- Relaciones semÃ¡nticas â†’ operaciones vectoriales
- DimensiÃ³n reducida comparado con vocabulario

### Similitud Coseno

La **similitud coseno** mide quÃ© tan similar es el significado de dos palabras calculando el coseno del Ã¡ngulo entre sus vectores:

```
cos(u, v) = (u Â· v) / (||u|| Ã— ||v||)
```

Donde:
- `u Â· v` es el producto punto
- `||u||` y `||v||` son las magnitudes (normas L2)

**InterpretaciÃ³n:**
- `+1`: Vectores idÃ©nticos (misma direcciÃ³n)
- `0`: Vectores ortogonales (no relacionados)
- `-1`: Vectores opuestos (antÃ³nimos)

### Word2Vec y Skip-gram

**Word2Vec** es un modelo que aprende embeddings entrenando una red neuronal simple para predecir contexto de palabras.

**Skip-gram** predice palabras del contexto dada una palabra central:
- Ventana de contexto: palabras cercanas
- Objetivo: maximizar la probabilidad de palabras de contexto
- Resultado: palabras que aparecen en contextos similares tienen embeddings similares

**ImplementaciÃ³n simplificada:**
```rust
// Para cada palabra central
for center_word in text {
    // Para cada palabra en su contexto
    for context_word in window(center_word) {
        // Acercar los embeddings
        embedding[center] += learning_rate * (embedding[context] - embedding[center])
    }
}
```

### AnalogÃ­as Vectoriales

Una propiedad fascinante de los embeddings es que permiten operaciones algebraicas que capturan relaciones semÃ¡nticas:

```
rey - hombre + mujer â‰ˆ reina
ParÃ­s - Francia + Italia â‰ˆ Roma
```

Esto funciona porque las relaciones semÃ¡nticas se codifican como direcciones en el espacio vectorial.

## âœ¨ CaracterÃ­sticas Implementadas

- âœ… **Matriz de embeddings** (lookup table)
- âœ… **Skip-gram simplificado** para entrenamiento
- âœ… **Similitud coseno** entre vectores
- âœ… **BÃºsqueda de vecinos cercanos** (palabras similares)
- âœ… **Operaciones de analogÃ­a** (word1 - word2 + word3 â‰ˆ ?)
- âœ… **NormalizaciÃ³n de embeddings**
- âœ… **Guardar/cargar embeddings** en JSON
- âœ… **Interfaz CLI completa**

## ğŸš€ CÃ³mo Ejecutar

### 1. Entrenar Embeddings

Entrena embeddings con el corpus de ejemplo usando Skip-gram:

```bash
cd days_15_21_llm/day18_embeddings
cargo run --release -- train
```

Esto generarÃ¡:
- Entrenamiento con 100 Ã©pocas
- Embeddings de 50 dimensiones
- Ventana de contexto de 2 palabras
- Archivo `embeddings.json` con los embeddings entrenados

### 2. Encontrar Palabras Similares

Busca las palabras mÃ¡s similares a una palabra dada:

```bash
cargo run --release -- similar king
cargo run --release -- similar cat 10  # Top 10 similares
```

Salida esperada:
```
ğŸ” Palabras similares a 'king':

  1. queen (similitud: 0.7234)
  2. prince (similitud: 0.6891)
  3. royal (similitud: 0.5432)
  ...
```

### 3. Operaciones de AnalogÃ­a

Realiza operaciones vectoriales del tipo "A es a B como C es a ?"

```bash
cargo run --release -- analogy king man woman
cargo run --release -- analogy happy good bad
```

Salida esperada:
```
ğŸ§® AnalogÃ­a: 'king' - 'man' + 'woman' â‰ˆ ?

  1. queen (similitud: 0.6543)
  2. princess (similitud: 0.5234)
  3. royal (similitud: 0.4321)
```

### 4. DemostraciÃ³n Completa

Ejecuta una demostraciÃ³n con varios ejemplos:

```bash
cargo run --release -- demo
```

## ğŸ“‚ Estructura del CÃ³digo

```
day18_embeddings/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # CLI y entrenamiento Skip-gram
â”‚   â”œâ”€â”€ embedding_layer.rs   # Matriz de embeddings y lookup
â”‚   â”œâ”€â”€ similarity.rs        # Similitud coseno y analogÃ­as
â”‚   â””â”€â”€ visualize.rs         # VisualizaciÃ³n (placeholder)
â”œâ”€â”€ Cargo.toml
â””â”€â”€ README.md
```

## âš ï¸ Nota Importante: SimplificaciÃ³n vs Word2Vec Real

### Â¿Por quÃ© esta implementaciÃ³n es tan simple?

Esta implementaciÃ³n es **intencionalmente simplificada** con fines educativos. A continuaciÃ³n se explican las diferencias con Word2Vec/Skip-gram real:

#### **Word2Vec Real (Mikolov et al. 2013)**

```
Arquitectura completa:
Input (one-hot) â†’ Embedding Layer â†’ Softmax â†’ Output (probabilidades)

CaracterÃ­sticas:
âœ“ Red neuronal de 2 capas
âœ“ FunciÃ³n de pÃ©rdida: Cross-entropy
âœ“ ActivaciÃ³n de salida: Softmax
âœ“ Backpropagation completa
âœ“ Negative sampling (5-20 palabras negativas)
âœ“ Learning rate adaptativo (decae con el tiempo)
âœ“ Optimizado para corpus grandes (millones de palabras)
```

#### **Nuestra ImplementaciÃ³n Simplificada**

```rust
// AproximaciÃ³n geomÃ©trica directa
let diff = &context_emb - &center_emb;
let new_center = &center_emb + &(&diff * learning_rate);
```

**CaracterÃ­sticas:**
- âŒ Sin red neuronal (solo operaciones vectoriales)
- âŒ Sin funciones de activaciÃ³n (ReLU, Sigmoid, Softmax)
- âŒ Sin negative sampling
- âœ… Learning rate fijo (0.01)
- âœ… ActualizaciÃ³n directa de embeddings

### Â¿Por quÃ© funciona esta simplificaciÃ³n?

Porque **captura la esencia de Skip-gram**: *palabras que aparecen en contextos similares deben tener embeddings cercanos*.

La versiÃ³n completa de Word2Vec hace esto a travÃ©s de:
- Maximizar `P(contexto|palabra_central)` usando gradientes
- Negative sampling para eficiencia

Nuestra versiÃ³n lo hace directamente:
- Acercar embeddings de palabras que co-ocurren
- Sin cÃ¡lculos probabilÃ­sticos complejos

### Decisiones de DiseÃ±o Justificadas

| Aspecto | DecisiÃ³n Tomada | RazÃ³n |
|---------|----------------|-------|
| **Sin activaciones** | Solo operaciones vectoriales | Word2Vec real tampoco usa activaciÃ³n en la capa de embedding. La softmax estÃ¡ en la salida, que nosotros evitamos |
| **Learning rate fijo** | 0.01 constante | Suficiente para corpus pequeÃ±o (~200 tokens). Word2Vec real usa decaimiento: `0.025 * (1 - epoch/max_epochs)` |
| **Sin negative sampling** | Solo muestras positivas | Con vocabulario pequeÃ±o (<100 palabras), no es crÃ­tico. Word2Vec real necesita esto para vocabularios de 100k+ palabras |
| **ActualizaciÃ³n directa** | Mover vectores geomÃ©tricamente | MÃ¡s intuitivo educativamente que backpropagation |

### ComparaciÃ³n de Resultados

| MÃ©trica | VersiÃ³n Simplificada | Word2Vec Real |
|---------|---------------------|---------------|
| **Velocidad** | âš¡ Muy rÃ¡pida | MÃ¡s lenta |
| **Corpus pequeÃ±o** | âœ… Excelente | Overkill |
| **Corpus grande** | âŒ Limitada | âœ… Superior |
| **Calidad embeddings** | Suficiente para ejemplos | Estado del arte |
| **Complejidad cÃ³digo** | ğŸ“ Educativa | ProducciÃ³n |

### Â¿CuÃ¡ndo usar cada versiÃ³n?

**Usar esta implementaciÃ³n simplificada:**
- âœ… Aprender conceptos de embeddings
- âœ… Corpus pequeÃ±os (<10,000 palabras)
- âœ… Prototipado rÃ¡pido
- âœ… Entender geometrÃ­a de embeddings

**Usar Word2Vec real (gensim, fastText):**
- âœ… ProducciÃ³n
- âœ… Corpus grandes (millones de palabras)
- âœ… MÃ¡xima calidad de embeddings
- âœ… Eficiencia con GPU

### Ejemplo de Skip-gram Real (referencia)

Para contexto educativo, asÃ­ se verÃ­a una implementaciÃ³n mÃ¡s realista:

```rust
// VersiÃ³n mÃ¡s cercana a Word2Vec real
fn train_skipgram_realistic(...) {
    let mut learning_rate = 0.025;

    for epoch in 0..epochs {
        // 1. Decaer learning rate
        learning_rate = 0.025 * (1.0 - epoch as f32 / epochs as f32);

        for (center_word, context_word) in pairs {
            // 2. Calcular score con dot product
            let score = center_emb.dot(&context_emb);

            // 3. Aplicar sigmoid
            let prob = 1.0 / (1.0 + (-score).exp());

            // 4. Gradiente positivo
            let gradient = (1.0 - prob) * learning_rate;
            center_emb += gradient * context_emb;

            // 5. Negative sampling (5 palabras aleatorias)
            for neg_word in sample_negative(5) {
                let neg_score = center_emb.dot(&neg_emb);
                let neg_prob = 1.0 / (1.0 + (-neg_score).exp());

                // Gradiente negativo (alejar)
                center_emb -= neg_prob * learning_rate * neg_emb;
            }
        }
    }
}
```

### ConclusiÃ³n

Esta implementaciÃ³n sacrifica **precisiÃ³n** y **escalabilidad** a favor de **claridad educativa** y **comprensiÃ³n conceptual**.

Para el DÃ­a 18, el objetivo es entender:
- âœ… QuÃ© son los embeddings
- âœ… CÃ³mo se representa significado en vectores
- âœ… Similitud coseno y operaciones vectoriales

**No** el objetivo es:
- âŒ Entrenar embeddings de producciÃ³n
- âŒ Competir con GloVe/fastText
- âŒ Escalar a millones de palabras

En los **DÃ­as 19-21** construiremos sobre estos embeddings para crear redes neuronales completas (MLP, Attention, Transformer) donde verÃ¡s activaciones, backpropagation y optimizaciÃ³n avanzada.

---

## ğŸ”¬ Detalles de ImplementaciÃ³n

### EmbeddingLayer (embedding_layer.rs)

```rust
pub struct EmbeddingLayer {
    pub embeddings: Array2<f32>,          // Matriz (vocab_size Ã— embedding_dim)
    pub token_to_id: HashMap<String, usize>,
    pub id_to_token: Vec<String>,
    pub embedding_dim: usize,
}
```

**MÃ©todos principales:**
- `new(vocab, dim)`: Crea embeddings con inicializaciÃ³n aleatoria
- `get_embedding(token)`: Obtiene vector de un token
- `update_embedding(token, vec)`: Actualiza embedding
- `normalize_embeddings()`: Normaliza todos los vectores a longitud 1
- `save(path)` / `load(path)`: Persistencia en JSON

### Similitud Coseno (similarity.rs)

```rust
pub fn cosine_similarity(u: &Array1<f32>, v: &Array1<f32>) -> f32 {
    let dot_product = u.iter().zip(v.iter()).map(|(a, b)| a * b).sum();
    let norm_u = u.iter().map(|x| x * x).sum::<f32>().sqrt();
    let norm_v = v.iter().map(|x| x * x).sum::<f32>().sqrt();
    dot_product / (norm_u * norm_v)
}
```

### Entrenamiento Skip-gram (main.rs)

```rust
fn train_skipgram(text: &str, embedding_dim: usize, epochs: usize, window_size: usize)
```

**Algoritmo:**
1. Tokenizar texto y construir vocabulario
2. Inicializar embeddings aleatoriamente
3. Para cada Ã©poca:
   - Para cada palabra central en el texto:
     - Obtener palabras de contexto (ventana)
     - Calcular gradiente simplificado
     - Actualizar embeddings de palabra central y contexto
4. Normalizar embeddings finales

## ğŸ“Š Ejemplos de Resultados

DespuÃ©s de entrenar con el corpus de ejemplo, se observan similitudes como:

| Par de Palabras | Similitud Coseno |
|----------------|------------------|
| king - queen   | 0.72 (alta)      |
| cat - dog      | 0.68 (alta)      |
| apple - banana | 0.65 (alta)      |
| car - bus      | 0.71 (alta)      |
| good - bad     | 0.42 (media)     |
| king - car     | 0.08 (baja)      |

**AnalogÃ­as exitosas:**
- `king - man + woman â‰ˆ queen` âœ“
- `apple - fruit + vehicle â‰ˆ car` âœ“

## ğŸ§ª Ejecutar Tests

```bash
cargo test
```

Los tests verifican:
- CreaciÃ³n de capa de embeddings
- Similitud coseno (vectores idÃ©nticos, ortogonales, opuestos)
- BÃºsqueda de vecinos cercanos
- NormalizaciÃ³n de vectores

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **ndarray**: Operaciones con matrices y vectores
- **ndarray-rand**: InicializaciÃ³n aleatoria de embeddings
- **rand**: GeneraciÃ³n de nÃºmeros aleatorios
- **serde/serde_json**: SerializaciÃ³n de embeddings
- **Rust std**: Collections (HashMap), I/O

## ğŸ“š Referencias

- **Lab 2.5: Experiment with Embeddings** - Google DeepMind AI Foundations
- Mikolov et al. (2013): "Efficient Estimation of Word Representations in Vector Space"
- Pennington et al. (2014): "GloVe: Global Vectors for Word Representation"

## ğŸ¯ PrÃ³ximos Pasos

Este proyecto es la base para:
- **DÃ­a 19**: MLP para clasificaciÃ³n de texto
- **DÃ­a 20**: Mecanismo de atenciÃ³n
- **DÃ­a 21**: Transformer completo

Los embeddings entrenados aquÃ­ se pueden usar como capa de entrada en redes neuronales mÃ¡s complejas.

---

**Parte del Plan Maestro de Aprendizaje de LLMs (DÃ­as 15-21)**

Proyecto educativo - ImplementaciÃ³n en Rust de conceptos fundamentales de NLP y LLMs.
