# ğŸ§  MLP para ClasificaciÃ³n de Texto

**DÃ­a 19 del DesafÃ­o de 21 DÃ­as** - Redes Neuronales para NLP

## DescripciÃ³n del Proyecto

ImplementaciÃ³n completa de un clasificador de texto usando **Multi-Layer Perceptron (MLP)** con capa de embeddings en Rust. Este proyecto demuestra cÃ³mo combinar embeddings de palabras con redes neuronales densas para realizar anÃ¡lisis de sentimientos.

El proyecto incluye una implementaciÃ³n desde cero de:
- **Capa de Embeddings**: RepresentaciÃ³n vectorial de palabras
- **MLP multi-capa**: Red neuronal con capas ocultas y activaciÃ³n ReLU
- **Cross-Entropy Loss**: FunciÃ³n de pÃ©rdida para clasificaciÃ³n multi-clase
- **Backpropagation**: Entrenamiento end-to-end de embeddings + MLP
- **Batch Processing**: Procesamiento eficiente por lotes

## CaracterÃ­sticas Implementadas

- âœ… MLP para clasificaciÃ³n de texto
- âœ… Capa de embedding trainable
- âœ… Batch processing con DataLoader
- âœ… Cross-entropy loss para clasificaciÃ³n multi-clase
- âœ… Tokenizador simple basado en palabras
- âœ… MÃ©tricas de evaluaciÃ³n (accuracy)
- âœ… Dataset sintÃ©tico de anÃ¡lisis de sentimientos (positivo/negativo/neutral)
- âœ… Backpropagation completa para embeddings y MLP

## Arquitectura del Modelo

```
Texto â†’ Tokenizador â†’ Embedding Layer â†’ MLP â†’ Softmax â†’ Clase
        (palabras)    (promedio)       (ReLU)  (probs)
```

**ConfiguraciÃ³n por defecto:**
- Embedding dimension: 32
- Hidden layers: [64, 32]
- Clases: 3 (positivo, negativo, neutral)
- Optimizer: SGD
- Learning rate: 0.01
- Batch size: 8
- Epochs: 50

## Estructura del CÃ³digo

```
day19_mlp_text/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # Punto de entrada con ejemplo de entrenamiento
â”‚   â”œâ”€â”€ mlp.rs               # ImplementaciÃ³n de MLP con backprop
â”‚   â”œâ”€â”€ text_classifier.rs   # Clasificador completo (Embedding + MLP)
â”‚   â””â”€â”€ batch.rs             # DataLoader, Batch, Tokenizer
â”œâ”€â”€ Cargo.toml
â””â”€â”€ README.md
```

### MÃ³dulos Principales

#### `mlp.rs`
- Struct `Layer`: Capa individual con pesos, biases y gradientes
- Struct `MLP`: Red multi-capa con forward/backward pass
- Funciones: `relu()`, `relu_derivative()`, `softmax()`, `sigmoid()`

#### `text_classifier.rs`
- Struct `EmbeddingLayer`: Capa de embeddings trainable
- Struct `TextClassifier`: Modelo completo embedding + MLP
- Funciones: `cross_entropy_loss()`, `cross_entropy_gradient()`

#### `batch.rs`
- Struct `Batch`: Lote de datos para entrenamiento
- Struct `DataLoader`: Iterador sobre batches con shuffle
- Struct `SimpleTokenizer`: Tokenizador basado en palabras
- FunciÃ³n: `average_embeddings()` - Promedia embeddings de tokens

## CÃ³mo Ejecutar

```bash
# Compilar y ejecutar
cd days_15_21_llm/day19_mlp_text
cargo run --release

# Ejecutar tests
cargo test

# Ver documentaciÃ³n
cargo doc --open
```

## Ejemplo de Salida

```
ğŸ§  MLP Text - DÃ­a 19: ClasificaciÃ³n de Texto con MLP
================================================

ğŸ“Š Creando dataset de sentimientos...
   - 36 ejemplos de entrenamiento
   - 6 ejemplos de prueba
   - Clases: ["positivo", "negativo", "neutral"]

ğŸ“ Construyendo vocabulario...
   - Vocabulario: 87 palabras

ğŸ—ï¸  Creando modelo...
   - Embedding dim: 32
   - Hidden layers: [64, 32]
   - Clases: 3

ğŸ“ Entrenando modelo...

Epoch   0 | Loss: 1.0986 | Train Acc: 33.33% | Test Acc: 33.33%
Epoch  10 | Loss: 0.7234 | Train Acc: 75.00% | Test Acc: 66.67%
Epoch  20 | Loss: 0.4521 | Train Acc: 88.89% | Test Acc: 83.33%
Epoch  30 | Loss: 0.2890 | Train Acc: 94.44% | Test Acc: 100.00%
Epoch  40 | Loss: 0.1876 | Train Acc: 97.22% | Test Acc: 100.00%
Epoch  49 | Loss: 0.1298 | Train Acc: 100.00% | Test Acc: 100.00%

âœ… Entrenamiento completado!

ğŸ”® Probando predicciones:

   "me encanta este producto es increÃ­ble"
   â†’ Clase predicha: 0 (positivo)

   "muy malo no lo recomiendo"
   â†’ Clase predicha: 1 (negativo)

   "estÃ¡ bien nada especial"
   â†’ Clase predicha: 2 (neutral)
```

## Conceptos TeÃ³ricos

### Embedding Layer
Una matriz de lookup que convierte IDs de tokens en vectores densos de dimensiÃ³n fija. Los embeddings se aprenden durante el entrenamiento mediante backpropagation.

**Forward pass:**
```rust
embedding_avg = promedio(embeddings[token_ids])
```

**Backward pass:**
```rust
grad_embeddings[token_id] += grad_output / num_tokens
```

### Multi-Layer Perceptron (MLP)
Red neuronal feedforward con capas totalmente conectadas. Cada capa aplica:
```
z = input Â· W + b
output = ReLU(z)  // en capas ocultas
```

### Cross-Entropy Loss
FunciÃ³n de pÃ©rdida para clasificaciÃ³n multi-clase:
```
L = -log(p_target)
```

Donde `p_target` es la probabilidad predicha para la clase correcta despuÃ©s de softmax.

**Gradiente simplificado (con softmax):**
```
grad = predictions - one_hot(targets)
```

### Backpropagation
Algoritmo para calcular gradientes y actualizar pesos:
1. Forward pass: calcular predicciones
2. Calcular pÃ©rdida
3. Backward pass: propagar gradientes desde salida hacia entrada
4. Actualizar pesos: `W = W - learning_rate Ã— grad_W`

## Dataset

El proyecto incluye un dataset sintÃ©tico de anÃ¡lisis de sentimientos en espaÃ±ol con:
- **36 ejemplos de entrenamiento** (12 por clase)
- **6 ejemplos de prueba** (2 por clase)
- **3 clases**: positivo, negativo, neutral

El dataset estÃ¡ diseÃ±ado para demostrar el funcionamiento del clasificador y puede ser reemplazado fÃ¡cilmente por datos reales.

## Extensiones Posibles

- [ ] Implementar optimizador Adam en lugar de SGD
- [ ] Agregar regularizaciÃ³n L2
- [ ] Visualizar curvas de aprendizaje con `plotters`
- [ ] Implementar dropout para prevenir overfitting
- [ ] Usar embeddings pre-entrenados (Word2Vec, GloVe)
- [ ] Concatenar embeddings en lugar de promediarlos
- [ ] AÃ±adir capa de atenciÃ³n antes del MLP
- [ ] Entrenar en datasets reales (IMDB, Yelp)

## Referencias

Basado en los conceptos de:
- Lab 3.1-3.4: Redes neuronales y MLP (Notebooks de Google DeepMind)
- TÃ©cnicas de embedding para NLP
- ClasificaciÃ³n de texto con deep learning

---

