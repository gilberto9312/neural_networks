# 游 Small Transformer LLM

D칤a 21 del Desaf칤o de 21 D칤as - Transformer Completo

## Descripci칩n del Proyecto

Implementaci칩n completa de un Small Language Model basado en arquitectura Transformer.

## Caracter칤sticas Implementadas

- [ ] Transformer encoder completo
- [ ] Transformer decoder completo
- [ ] Layer Normalization
- [ ] Feed-forward network
- [ ] Sistema de entrenamiento completo
- [ ] Generaci칩n de texto autoregresiva
- [ ] Checkpoint saving/loading
- [ ] M칠tricas de evaluaci칩n (loss, perplexity)

## C칩mo Ejecutar

```bash
cd days_15_21_llm/day21_small_transformer

# Entrenar modelo
cargo run --release -- train

# Generar texto
cargo run --release -- generate "Abeni was"
```

## Arquitectura

- Vocab size: 5000-8000 tokens (BPE)
- Embedding dim: 128
- Hidden dim: 256
- Num heads: 4
- Num layers: 2 (encoder) + 2 (decoder)
- Total params: ~3-4M

## Dataset

Africa Galore (232 p치rrafos)

---

**Nota**: Este proyecto es el objetivo final del plan maestro de aprendizaje de LLMs (d칤as 15-21).
