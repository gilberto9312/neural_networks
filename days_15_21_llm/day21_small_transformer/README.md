# ğŸš€ Small Transformer LLM - DÃ­a 21

**Proyecto Final del Plan de 21 DÃ­as de Aprendizaje de LLMs**

ImplementaciÃ³n educativa completa de un Transformer Language Model (decoder-only, estilo GPT) desde cero en Rust, sin usar librerÃ­as de ML de alto nivel.

## ğŸ“‹ DescripciÃ³n del Proyecto

Este es el proyecto culminante del plan de 21 dÃ­as, integrando todos los conceptos aprendidos:
- **DÃ­as 15-16**: N-grams y Perplexity
- **DÃ­a 17**: TokenizaciÃ³n (BPE)
- **DÃ­a 18**: Word Embeddings
- **DÃ­a 19**: MLPs para Texto
- **DÃ­a 20**: Mecanismo de AtenciÃ³n
- **DÃ­a 21**: **Transformer Completo** â† ESTÃS AQUÃ

## âœ… CaracterÃ­sticas Implementadas

- âœ… **Multi-Head Attention** con mÃ¡scara causal
- âœ… **Positional Encoding** sinusoidal
- âœ… **Feed-Forward Networks** con activaciÃ³n ReLU
- âœ… **Layer Normalization**
- âœ… **Residual Connections**
- âœ… **Decoder Stack** (2 capas)
- âœ… **Embedding Layer**
- âœ… **GeneraciÃ³n autoregressiva** de texto
- âœ… **Tokenizer simple** basado en palabras
- âœ… **Loop de entrenamiento** (forward pass + mÃ©tricas)
- âœ… **Checkpoint saving/loading** (configuraciÃ³n y tokenizer)
- âœ… **CLI funcional** con mÃºltiples comandos

## ğŸ—ï¸ Arquitectura del Modelo

```
TransformerLM (Decoder-Only, GPT-style)
â”œâ”€â”€ Embedding Layer (vocab_size â†’ d_model)
â”œâ”€â”€ Positional Encoding (sinusoidal)
â”œâ”€â”€ Decoder Stack (2 capas)
â”‚   â”œâ”€â”€ Masked Multi-Head Attention (4 cabezas)
â”‚   â”œâ”€â”€ Layer Normalization
â”‚   â”œâ”€â”€ Feed-Forward Network (d_model â†’ 4*d_model â†’ d_model)
â”‚   â””â”€â”€ Layer Normalization
â””â”€â”€ Output Projection (d_model â†’ vocab_size)
```

### ParÃ¡metros del Modelo

- **Vocab size**: ~30 tokens (simplificado para demostraciÃ³n)
- **Embedding dim**: 128
- **Num heads**: 4
- **Num layers**: 2 (decoder)
- **d_ff**: 512 (4 Ã— d_model)
- **Max seq len**: 64
- **Total params**: ~403,200

## ğŸš€ CÃ³mo Ejecutar

### Compilar el Proyecto

```bash
cd days_15_21_llm/day21_small_transformer
cargo build --release
```

### Comandos Disponibles

#### 1. DemostraciÃ³n Completa

```bash
cargo run --release -- demo
```

Ejecuta una demostraciÃ³n completa que:
1. Construye el vocabulario desde el corpus
2. Crea el modelo transformer
3. Prepara el dataset
4. Simula entrenamiento (forward pass + mÃ©tricas)
5. Genera texto desde mÃºltiples prompts

#### 2. InformaciÃ³n del Modelo

```bash
cargo run --release -- info
```

Muestra la arquitectura y configuraciÃ³n del modelo.

#### 3. Generar Texto

```bash
cargo run --release -- generate "Abeni was"
cargo run --release -- generate "The marketplace"
cargo run --release -- generate "Children sang"
```

Genera texto de forma autoregressiva desde un prompt dado.

#### 4. Entrenar Modelo

```bash
cargo run --release -- train
```

Ejecuta el loop de entrenamiento (50 Ã©pocas) y guarda la configuraciÃ³n.

## ğŸ“‚ Estructura del Proyecto

```
src/
â”œâ”€â”€ main.rs              # CLI y funciones principales
â”œâ”€â”€ attention.rs         # Multi-Head Attention
â”œâ”€â”€ positional.rs        # Positional Encoding
â”œâ”€â”€ feedforward.rs       # Feed-Forward Networks
â”œâ”€â”€ layer_norm.rs        # Layer Normalization
â”œâ”€â”€ embedding.rs         # Embedding Layer
â”œâ”€â”€ encoder.rs           # Encoder (no usado en LM puro)
â”œâ”€â”€ decoder.rs           # Decoder Stack
â”œâ”€â”€ transformer.rs       # Arquitectura completa
â”œâ”€â”€ tokenizer.rs         # Tokenizer simple
â”œâ”€â”€ dataset.rs           # Dataset loader
â”œâ”€â”€ training.rs          # Loop de entrenamiento
â”œâ”€â”€ generation.rs        # GeneraciÃ³n autoregressiva
â”œâ”€â”€ checkpoint.rs        # Guardar/cargar configuraciÃ³n
â””â”€â”€ utils.rs             # Utilidades (softmax, loss, etc.)
```

## ğŸ§® Ecuaciones Clave Implementadas

### Scaled Dot-Product Attention

```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V
```

### Positional Encoding

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

### Layer Normalization

```
LayerNorm(x) = Î³ * (x - Î¼) / (Ïƒ + Îµ) + Î²
```

### Feed-Forward Network

```
FFN(x) = max(0, xW1 + b1)W2 + b2
```

## ğŸ“Š Dataset

El proyecto usa un extracto de **Africa Galore** integrado en `dataset.rs`, que incluye:
- 10 pÃ¡rrafos de ejemplo
- Temas: arte, historia, deporte, cultura, naturaleza
- ~200 palabras Ãºnicas
- Vocabulario final: ~30 tokens (con filtrado de min_freq=2)

## âš™ï¸ Detalles de ImplementaciÃ³n

### Â¿QuÃ© EstÃ¡ Completamente Implementado?

âœ… **Forward Pass Completo**:
- Embeddings â†’ Positional Encoding â†’ Decoder â†’ Output Projection
- Multi-Head Attention con mÃ¡scaras causales
- Feed-Forward Networks con ReLU
- Layer Normalization
- Residual Connections

âœ… **GeneraciÃ³n Autoregressiva**:
- Sampling con temperatura
- PredicciÃ³n greedy
- Manejo de secuencias de longitud variable

âœ… **MÃ©tricas**:
- Cross-Entropy Loss
- Perplexity

### âš ï¸ Limitaciones (ImplementaciÃ³n Educativa)

Este es un proyecto **educativo simplificado**. Para un transformer completo en producciÃ³n se requerirÃ­a:

1. **Backpropagation Completa** (~1500+ lÃ­neas adicionales)
   - Chain rule para todas las capas
   - Gradientes para attention, feedforward, layer norm
   - ActualizaciÃ³n de pesos

2. **Optimizador Adam** (~300+ lÃ­neas)
   - First moment estimation
   - Second moment estimation
   - Bias correction

3. **CaracterÃ­sticas Adicionales**:
   - Learning rate scheduling
   - Gradient clipping
   - Dropout
   - Weight decay
   - Batching eficiente
   - Mixed precision training

**Total estimado**: ~3000+ lÃ­neas de cÃ³digo adicional para entrenamiento real.

## ğŸ¯ Conceptos Demostrados

Este proyecto demuestra exitosamente:

1. âœ… **Arquitectura Transformer completa** (decoder-only)
2. âœ… **Self-Attention** con mÃºltiples cabezas
3. âœ… **Positional Information** mediante encoding sinusoidal
4. âœ… **MÃ¡scaras Causales** para prevenir ver el futuro
5. âœ… **NormalizaciÃ³n y Residuales** para entrenamiento estable
6. âœ… **GeneraciÃ³n Autoregressiva** token por token
7. âœ… **Pipeline Completo** desde texto â†’ tokens â†’ modelo â†’ texto

## ğŸ“š Aprendizajes del Plan de 21 DÃ­as

### DÃ­as 1-14: Fundamentos de Redes Neuronales
- Neuronas, activaciones, backpropagation
- Optimizadores (SGD, Momentum, Adam)
- RegularizaciÃ³n (L1, L2, Dropout)
- CNNs para imÃ¡genes (MNIST)

### DÃ­as 15-21: LLMs y Transformers
- **DÃ­a 15-16**: N-grams y mÃ©tricas de lenguaje
- **DÃ­a 17**: TokenizaciÃ³n con BPE
- **DÃ­a 18**: Word Embeddings (Skip-gram)
- **DÃ­a 19**: MLPs para clasificaciÃ³n de texto
- **DÃ­a 20**: Mecanismo de AtenciÃ³n
- **DÃ­a 21**: **Transformer Completo** âœ¨

## ğŸ” Testing

Ejecutar tests:

```bash
cargo test
```

## ğŸ“– Referencias

Este proyecto transpila y adapta los conceptos de los notebooks de Google DeepMind AI Foundations:
- Lab 4.1: Attention Visualization
- Lab 4.2: Implement Attention Equation
- Lab 4.3: Masked Multi-Head Attention
- Lab 4.4: Positional Embeddings
- Lab 1.5: Train Your Own Small Language Model
- Lab 2.6: Train SLM with BPE Tokenizer

## ğŸ“ Para Estudiantes

Este proyecto es ideal para:
- Entender **cÃ³mo funciona un transformer** internamente
- Aprender **implementaciÃ³n desde cero** sin abstracciones mÃ¡gicas
- Ver **todas las piezas del puzzle** en un solo lugar
- Experimentar con arquitecturas pequeÃ±as y rÃ¡pidas

## ğŸ™ Agradecimientos

Proyecto educativo basado en:
- Google DeepMind AI Foundations Course
- "Attention Is All You Need" (Vaswani et al., 2017)
- The Illustrated Transformer (Jay Alammar)

---

**ğŸ‰ FELICITACIONES: Has completado el plan de 21 dÃ­as de aprendizaje de LLMs desde cero!**
