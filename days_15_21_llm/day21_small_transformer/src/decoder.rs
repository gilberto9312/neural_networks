// Transformer Decoder

// TODO: Implementar decoder
// - Masked self-attention
// - Cross-attention
// - Feed-forward network
// - Layer normalization
