// Small Transformer LLM
// DÃ­a 21: Transformer Completo - Proyecto Final del Plan de 21 DÃ­as

mod attention;
mod positional;
mod feedforward;
mod layer_norm;
mod utils;
mod embedding;
mod encoder;
mod decoder;
mod transformer;
mod tokenizer;
mod dataset;
mod training;
mod generation;
mod checkpoint;

use std::env;
use transformer::TransformerLM;
use tokenizer::SimpleTokenizer;
use dataset::{TextDataset, AFRICA_GALORE_SAMPLE};
use checkpoint::ModelConfig;

fn main() {
    let args: Vec<String> = env::args().collect();

    if args.len() < 2 {
        print_usage();
        return;
    }

    match args[1].as_str() {
        "demo" => demo(),
        "train" => train_model(),
        "generate" => {
            if args.len() < 3 {
                eprintln!("âŒ Uso: cargo run --release -- generate \"<prompt>\"");
                return;
            }
            let prompt = &args[2];
            generate_text(prompt);
        }
        "info" => show_info(),
        _ => {
            eprintln!("âŒ Comando desconocido: {}", args[1]);
            print_usage();
        }
    }
}

fn print_usage() {
    println!("ğŸš€ Small Transformer LLM - DÃ­a 21");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("\nProyecto final del plan de 21 dÃ­as de aprendizaje de LLMs\n");
    println!("COMANDOS:");
    println!("  demo                     DemostraciÃ³n completa");
    println!("  train                    Entrenar modelo");
    println!("  generate \"<prompt>\"      Generar texto desde prompt");
    println!("  info                     InformaciÃ³n del modelo\n");
    println!("EJEMPLOS:");
    println!("  cargo run --release -- demo");
    println!("  cargo run --release -- generate \"Abeni was\"");
    println!("  cargo run --release -- train");
}

fn demo() {
    println!("\nğŸ¯ DEMOSTRACIÃ“N COMPLETA - Small Transformer LLM");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // 1. Preparar datos
    println!("ğŸ“š Paso 1/5: Preparando datos...");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let corpus = vec![AFRICA_GALORE_SAMPLE.to_string()];
    let tokenizer = SimpleTokenizer::from_corpus(&corpus, 2);

    println!("âœ“ Vocabulario construido: {} tokens", tokenizer.vocab_size());
    println!("âœ“ Tokens especiales: PAD={}, UNK={}, BOS={}, EOS={}",
        tokenizer.pad_token_id,
        tokenizer.unk_token_id,
        tokenizer.bos_token_id,
        tokenizer.eos_token_id
    );

    // 2. Crear modelo
    println!("\nğŸ—ï¸  Paso 2/5: Creando modelo transformer...");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let model = TransformerLM::new(
        tokenizer.vocab_size(),
        128,  // d_model
        4,    // num_heads
        2,    // num_layers
        64,   // max_seq_len
    );

    println!("{}", model.info());

    // 3. Preparar dataset
    println!("\nğŸ“Š Paso 3/5: Preparando dataset...");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let dataset = TextDataset::from_text(AFRICA_GALORE_SAMPLE, &tokenizer, 32);
    println!("âœ“ Dataset creado: {} secuencias de longitud 32", dataset.len());

    // 4. Entrenamiento (simulado)
    println!("\nğŸ“ Paso 4/5: DemostraciÃ³n de entrenamiento...");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    training::train(&model, &dataset, 20, 4);

    // 5. GeneraciÃ³n
    println!("\nâœ¨ Paso 5/5: GeneraciÃ³n de texto...");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let prompts = vec![
        "Abeni was",
        "The marketplace",
        "Children sang",
    ];

    for prompt in prompts {
        println!("\nğŸ“ Prompt: \"{}\"", prompt);
        let generated = generation::generate(&model, &tokenizer, prompt, 15, 1.0);
        println!("   â†’ {}", generated);
    }

    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("âœ… DEMOSTRACIÃ“N COMPLETADA");
    println!("\nğŸ’¡ CONCEPTOS DEMOSTRADOS:");
    println!("   1. TokenizaciÃ³n con vocabulario basado en palabras");
    println!("   2. Arquitectura Transformer (decoder-only)");
    println!("   3. Multi-Head Attention con mÃ¡scara causal");
    println!("   4. Positional Encoding sinusoidal");
    println!("   5. Feed-Forward Networks y Layer Normalization");
    println!("   6. GeneraciÃ³n autoregressiva de texto");
    println!("\nâš ï¸  NOTA: Este es un modelo educativo simplificado.");
    println!("   Para producciÃ³n se requerirÃ­a entrenamiento con backprop completo.");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
}

fn train_model() {
    println!("\nğŸ“ MODO ENTRENAMIENTO");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let corpus = vec![AFRICA_GALORE_SAMPLE.to_string()];
    let tokenizer = SimpleTokenizer::from_corpus(&corpus, 2);
    let model = TransformerLM::new(tokenizer.vocab_size(), 128, 4, 2, 64);
    let dataset = TextDataset::from_text(AFRICA_GALORE_SAMPLE, &tokenizer, 32);

    training::train(&model, &dataset, 50, 8);

    println!("\nğŸ’¾ Guardando configuraciÃ³n...");
    let config = ModelConfig {
        vocab_size: model.vocab_size,
        d_model: model.d_model,
        num_heads: model.num_heads,
        num_layers: model.num_layers,
        max_seq_len: model.max_seq_len,
    };

    if let Err(e) = checkpoint::save_config(&config, "model_config.json") {
        eprintln!("âš ï¸  Error al guardar configuraciÃ³n: {}", e);
    } else {
        println!("âœ“ ConfiguraciÃ³n guardada en model_config.json");
    }

    if let Err(e) = checkpoint::save_tokenizer(&tokenizer, "tokenizer.json") {
        eprintln!("âš ï¸  Error al guardar tokenizer: {}", e);
    } else {
        println!("âœ“ Tokenizer guardado en tokenizer.json");
    }
}

fn generate_text(prompt: &str) {
    println!("\nâœ¨ MODO GENERACIÃ“N");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let corpus = vec![AFRICA_GALORE_SAMPLE.to_string()];
    let tokenizer = SimpleTokenizer::from_corpus(&corpus, 2);
    let model = TransformerLM::new(tokenizer.vocab_size(), 128, 4, 2, 64);

    println!("\nPrompt: \"{}\"", prompt);
    println!("Generando 20 tokens...\n");

    let generated = generation::generate(&model, &tokenizer, prompt, 20, 1.0);

    println!("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!("RESULTADO:");
    println!("{}", generated);
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
}

fn show_info() {
    println!("\nğŸ“Š INFORMACIÃ“N DEL MODELO");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    let corpus = vec![AFRICA_GALORE_SAMPLE.to_string()];
    let tokenizer = SimpleTokenizer::from_corpus(&corpus, 2);
    let model = TransformerLM::new(tokenizer.vocab_size(), 128, 4, 2, 64);

    println!("{}\n", model.info());

    println!("ARQUITECTURA:");
    println!("  â€¢ Decoder-only (GPT-style)");
    println!("  â€¢ {} capas decoder", model.num_layers);
    println!("  â€¢ {} cabezas de atenciÃ³n", model.num_heads);
    println!("  â€¢ DimensiÃ³n por cabeza: {}", model.d_model / model.num_heads);
    println!("  â€¢ DimensiÃ³n feed-forward: {}", model.d_ff);
    println!("\nCOMPONENTES:");
    println!("  âœ“ Embedding Layer");
    println!("  âœ“ Positional Encoding (sinusoidal)");
    println!("  âœ“ Multi-Head Attention con mÃ¡scara causal");
    println!("  âœ“ Feed-Forward Networks (ReLU)");
    println!("  âœ“ Layer Normalization");
    println!("  âœ“ Residual Connections");
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
}
