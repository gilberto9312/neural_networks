// Loop de entrenamiento simplificado

use crate::transformer::TransformerLM;
use crate::dataset::TextDataset;
use crate::utils::{cross_entropy_loss, perplexity};

/// Entrena el modelo (versiÃ³n simplificada sin backprop completo)
///
/// NOTA: Esta es una implementaciÃ³n educativa simplificada.
/// Un entrenamiento real requerirÃ­a backpropagation completo y optimizador Adam.
///
/// # Argumentos
/// * `model` - Modelo a entrenar (nota: tomado por valor, no modifica el original)
/// * `dataset` - Dataset de entrenamiento
/// * `epochs` - NÃºmero de Ã©pocas
/// * `batch_size` - TamaÃ±o del batch
pub fn train(
    model: &TransformerLM,
    dataset: &TextDataset,
    epochs: usize,
    batch_size: usize,
) {
    println!("\nðŸŽ“ Iniciando entrenamiento");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("Ã‰pocas: {}", epochs);
    println!("Batch size: {}", batch_size);
    println!("Dataset size: {}", dataset.len());
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    for epoch in 0..epochs {
        let mut epoch_loss = 0.0;
        let mut num_batches = 0;

        // Entrenar en batches
        for batch_idx in 0..10 {  // Limitado a 10 batches por Ã©poca para velocidad
            if let Some((inputs, targets)) = dataset.get_batch(batch_size) {
                let mut batch_loss = 0.0;

                for (input_seq, target_seq) in inputs.iter().zip(targets.iter()) {
                    // Forward pass
                    let logits = model.forward(input_seq);

                    // Calcular loss
                    let loss = cross_entropy_loss(&logits, target_seq);
                    batch_loss += loss;

                    // NOTA: AquÃ­ irÃ­a backpropagation en una implementaciÃ³n completa
                    // Por simplicidad educativa, solo calculamos mÃ©tricas
                }

                epoch_loss += batch_loss / inputs.len() as f32;
                num_batches += 1;
            }
        }

        let avg_loss = epoch_loss / num_batches.max(1) as f32;
        let ppl = perplexity(avg_loss);

        if epoch % 10 == 0 || epoch == epochs - 1 {
            println!(
                "Ã‰poca {:3}/{} | Loss: {:.4} | Perplexity: {:.2}",
                epoch + 1,
                epochs,
                avg_loss,
                ppl
            );
        }
    }

    println!("\nâœ… Entrenamiento completado");
    println!("\nâš ï¸  NOTA IMPORTANTE:");
    println!("Esta es una demostraciÃ³n educativa simplificada.");
    println!("El modelo NO ha sido actualizado (falta implementaciÃ³n de backprop).");
    println!("Para un transformer funcional completo, se requerirÃ­a:");
    println!("  1. Backpropagation completa con chain rule");
    println!("  2. Optimizador Adam con moment estimation");
    println!("  3. Gradient clipping y learning rate scheduling");
    println!("  4. ImplementaciÃ³n serÃ­a ~3000+ lÃ­neas adicionales de cÃ³digo");
}

/// Calcula mÃ©tricas en dataset de validaciÃ³n
pub fn evaluate(model: &TransformerLM, dataset: &TextDataset) -> (f32, f32) {
    let batch_size = 8;
    let mut total_loss = 0.0;
    let mut num_samples = 0;

    for _ in 0..5 {  // Evaluar en 5 batches
        if let Some((inputs, targets)) = dataset.get_batch(batch_size) {
            for (input_seq, target_seq) in inputs.iter().zip(targets.iter()) {
                let logits = model.forward(input_seq);
                let loss = cross_entropy_loss(&logits, target_seq);
                total_loss += loss;
                num_samples += 1;
            }
        }
    }

    let avg_loss = total_loss / num_samples.max(1) as f32;
    let ppl = perplexity(avg_loss);

    (avg_loss, ppl)
}
