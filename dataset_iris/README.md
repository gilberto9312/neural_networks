# ğŸŒ¸ Red Neuronal para ClasificaciÃ³n del Dataset Iris ğŸŒ¸

Una implementaciÃ³n de una red neuronal en Rust desde cero, diseÃ±ada para resolver el clÃ¡sico problema de clasificaciÃ³n multiclase del dataset Iris.

## ğŸ¯ DescripciÃ³n

Este proyecto construye, entrena y evalÃºa una red neuronal feedforward para clasificar las flores de Iris en tres especies diferentes (Setosa, Versicolor y Virginica) basÃ¡ndose en 3 caracterÃ­sticas fÃ­sicas: longitud, ancho de sÃ©palos y pÃ©talos.

El programa realiza las siguientes tareas:
1.  **Carga y Preprocesa los Datos**: Lee el archivo `dataset.csv`, normaliza las caracterÃ­sticas usando escalado min-max y divide los datos en conjuntos de entrenamiento y prueba.
2.  **Entrena el Modelo**: Utiliza backpropagation y el optimizador Adam para ajustar los pesos de la red.
3.  **Genera una GrÃ¡fica**: Crea un archivo `training_curve.png` que muestra la evoluciÃ³n del error (pÃ©rdida) durante el entrenamiento.
4.  **EvalÃºa el Rendimiento**: Mide la precisiÃ³n del modelo entrenado en el conjunto de prueba.

## ğŸ—ï¸ Arquitectura de la Red

-   **Capa de entrada**: 4 neuronas (corresponden a las 4 caracterÃ­sticas del dataset).
-   **Capas ocultas**: 2 capas ocultas con 16 y 8 neuronas respectivamente.
-   **FunciÃ³n de activaciÃ³n (oculta)**: **ReLU (Rectified Linear Unit)**.
-   **Capa de salida**: 3 neuronas (una para cada especie de Iris).
-   **FunciÃ³n de activaciÃ³n (salida)**: **Softmax**, para obtener una distribuciÃ³n de probabilidad sobre las clases.

## ğŸ§  FÃ³rmulas Clave

### 1. FunciÃ³n de ActivaciÃ³n ReLU

Para las capas ocultas, se utiliza la funciÃ³n ReLU, que introduce no linealidad y es computacionalmente eficiente.

```
ReLU(x) = max(0, x)
```

Su derivada es simple, lo que acelera el proceso de backpropagation:

```
ReLU'(x) = 1 si x > 0, 0 si x â‰¤ 0
```

### 2. FunciÃ³n de ActivaciÃ³n Softmax

En la capa de salida, Softmax convierte las salidas brutas (logits) en probabilidades, asegurando que la suma de todas las probabilidades sea 1.

```
Softmax(z_i) = e^(z_i) / Î£(e^(z_j))
```

### 3. FunciÃ³n de PÃ©rdida (Cross-Entropy)

Para medir el error en problemas de clasificaciÃ³n multiclase, se utiliza la pÃ©rdida de entropÃ­a cruzada. Compara la distribuciÃ³n de probabilidad predicha con la distribuciÃ³n real (one-hot encoded).

```
L = -Î£(y_true * log(y_pred))
```

### 4. Optimizador Adam

Adam (Adaptive Moment Estimation) es un algoritmo de optimizaciÃ³n eficiente que ajusta la tasa de aprendizaje para cada peso de la red. Combina las ventajas de dos extensiones del descenso de gradiente estocÃ¡stico: Momentum y RMSprop.

```
m = Î²â‚Â·m + (1-Î²â‚)Â·g          // EstimaciÃ³n del primer momento (media)
v = Î²â‚‚Â·v + (1-Î²â‚‚)Â·gÂ²         // EstimaciÃ³n del segundo momento (varianza)
m_hat = m / (1 - Î²â‚^t)       // CorrecciÃ³n de sesgo
v_hat = v / (1 - Î²â‚‚^t)
w = w - Î· Â· m_hat / (âˆšv_hat + Îµ)
```

## ğŸ“Š Salida del Programa

```
ğŸŒ¸ Red Neuronal para Dataset Iris ğŸŒ¸
Cargando dataset...
Dataset cargado exitosamente!
Entrenamiento: 120 muestras
Prueba: 30 muestras
Entrenamiento: 105 muestras
ValidaciÃ³n: 15 muestras
Iniciando entrenamiento...
Epoch 0: Train Loss = 1.1119, Val Loss = 1.1193, Accuracy = 32.38%
Epoch 100: Train Loss = 0.3691, Val Loss = 0.3599, Accuracy = 97.14%
Epoch 200: Train Loss = 0.1847, Val Loss = 0.1699, Accuracy = 98.10%
...
Epoch 9900: Train Loss = 0.0139, Val Loss = 0.0063, Accuracy = 100.00%

ğŸ“Š Generando grÃ¡fica de entrenamiento...

ğŸ¯ Evaluando en conjunto de prueba...
PrecisiÃ³n en prueba: 100.00%
PrecisiÃ³n en entrenamiento: 99.17%

ğŸ” Ejemplo de predicciÃ³n:
Muestra: [0.2222, 0.6250, 0.0677, 0.0416]
Clase real: 0 (Setosa)
Clase predicha: 0 (Setosa)
Probabilidades: [0.99, 0.01, 0.00]
```


## ğŸ”§ ExperimentaciÃ³n y Variabilidad de Resultados

Los resultados de una red neuronal son muy sensibles a su configuraciÃ³n. Puedes obtener diferentes resultados si modificas los siguientes hiperparÃ¡metros en `main.rs`:

### 1. NÃºmero de Neuronas
Cambiar el tamaÃ±o de las capas ocultas en la funciÃ³n `IrisNeuralNetwork::new()` puede afectar la capacidad del modelo.
-   **Menos neuronas**: Puede llevar a *underfitting* (el modelo es demasiado simple para capturar la complejidad de los datos).
-   **MÃ¡s neuronas**: Puede llevar a *overfitting* (el modelo memoriza los datos de entrenamiento y no generaliza bien a datos nuevos), ademÃ¡s de aumentar el tiempo de entrenamiento.

```rust
// En IrisNeuralNetwork::new()
let layer_sizes = vec![4, 16, 8, 3]; // Prueba cambiando 16 y 8
```

### 2. Tasa de Aprendizaje (Learning Rate)
La tasa de aprendizaje (`learning_rate`) controla cuÃ¡n grandes son los ajustes a los pesos en cada iteraciÃ³n.
-   **Tasa muy alta**: El modelo puede converger rÃ¡pidamente pero corre el riesgo de "pasarse" del mÃ­nimo Ã³ptimo, provocando inestabilidad.
-   **Tasa muy baja**: El entrenamiento serÃ¡ mÃ¡s lento y puede quedarse atascado en mÃ­nimos locales.

```rust
// En main()
let mut nn = IrisNeuralNetwork::new(0.001); // Prueba con 0.01, 0.0005, etc.
```

### 3. FunciÃ³n de ActivaciÃ³n (ReLU vs. Sigmoid)
Actualmente, el modelo usa **ReLU**. Si la cambias por **Sigmoid**, el comportamiento del entrenamiento cambiarÃ¡.
-   **Sigmoid**: Comprime los valores a un rango entre 0 y 1. Es Ãºtil pero puede sufrir del "problema de desvanecimiento del gradiente" (vanishing gradient), donde los gradientes se vuelven muy pequeÃ±os, ralentizando o deteniendo el aprendizaje, especialmente en redes profundas.
-   **ReLU**: Es menos susceptible a este problema y a menudo lleva a una convergencia mÃ¡s rÃ¡pida.

Para cambiar a Sigmoid, deberÃ¡s modificar la llamada a la funciÃ³n de activaciÃ³n en el mÃ©todo `forward()` y la de su derivada en `backward()`.

```rust
// En IrisNeuralNetwork::forward()
// Cambia esto:
layer_output = layer_output.iter().map(|&x| relu(x)).collect();
// Por esto:
layer_output = layer_output.iter().map(|&x| sigmoid(x)).collect();

// Y en IrisNeuralNetwork::backward()
// Cambia esto:
next_delta[j] *= relu_derivative(layer_input[j]);
// Por esto:
next_delta[j] *= sigmoid_derivative(layer_input[j]);
```
