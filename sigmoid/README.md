# ğŸ§  Redes Neuronales - Serie de Aprendizaje Diario

## ğŸ“… DÃ­a 2: FunciÃ³n Sigmoid

### ğŸ¯ Objetivo del DÃ­a
Entender cÃ³mo la funciÃ³n sigmoid transforma las salidas lineales de una neurona en salidas no-lineales, y por quÃ© esto es fundamental para el funcionamiento de las redes neuronales.

### ğŸ” Â¿QuÃ© es la FunciÃ³n Sigmoid?

La funciÃ³n sigmoid es una funciÃ³n matemÃ¡tica que toma cualquier nÃºmero real y lo "comprime" a un valor entre 0 y 1. Su fÃ³rmula es:

```
Ïƒ(z) = 1 / (1 + e^(-z))
```

#### Propiedades clave:
- **Rango**: (0, 1) - nunca llega exactamente a 0 o 1
- **Forma**: Curva suave en forma de "S"
- **Punto medio**: Ïƒ(0) = 0.5
- **AsÃ­ntotas**: Cuando z â†’ +âˆ, Ïƒ(z) â†’ 1; cuando z â†’ -âˆ, Ïƒ(z) â†’ 0
- **Derivable**: En todos los puntos, lo que permite el entrenamiento por gradiente

### ğŸ—ï¸ Estructura del CÃ³digo

#### `struct Neuron`
Representa una neurona artificial con:
- `synapses`: Vector de pesos sinÃ¡pticos (conexiones)
- `threshold`: Umbral o bias de la neurona

#### MÃ©todos principales:
1. **`salida_lineal()`**: Calcula z = Î£(wi * xi) + b
2. **`salida_con_sigmoid()`**: Aplica sigmoid a la salida lineal
3. **`sigmoid()`**: Implementa la funciÃ³n matemÃ¡tica sigmoid

### ğŸ§ª Experimentos Realizados

#### 1. AnÃ¡lisis TeÃ³rico
- ExploraciÃ³n de las propiedades matemÃ¡ticas de sigmoid
- Prueba con valores desde -10 hasta +10
- ObservaciÃ³n de cÃ³mo los valores extremos se saturan

#### 2. ComparaciÃ³n PrÃ¡ctica
- **Salida Lineal**: Puede ser cualquier valor real (negativo, positivo, grande)
- **Salida Sigmoid**: Siempre entre 0 y 1, interpretable como probabilidad

### ğŸ’¡ Conceptos Clave Aprendidos

1. **TransformaciÃ³n no-lineal**: Sigmoid convierte la combinaciÃ³n lineal en una salida no-lineal
2. **SaturaciÃ³n**: Valores muy grandes o muy pequeÃ±os se "saturan" cerca de 1 o 0
3. **Suavidad**: La curva continua permite calcular gradientes para el entrenamiento
4. **Interpretabilidad**: La salida puede interpretarse como una probabilidad
5. **ActivaciÃ³n**: Determina cuÃ¡ndo la neurona "se activa" (valores cercanos a 1)

### ğŸ”§ CÃ³mo Ejecutar

```bash
# Compilar y ejecutar
cargo run

# O si usas rustc directamente
rustc main.rs && ./main
```

### ğŸ“Š Ejemplo de Salida

```
ğŸ§  REDES NEURONALES - DÃA: FUNCIÃ“N SIGMOID
===========================================

ğŸ“Š COMPARACIÃ“N: LINEAL vs NO-LINEAL
Entradas             | Salida Lineal  | Salida Sigmoid
-------------------------------------------------------
[1.0, 2.0, -1.0]    |     -0.1000    |     0.475021
[-2.0, 1.5, 3.0]    |     1.1500     |     0.759469
[0.0, 0.0, 0.0]     |     0.1000     |     0.524979
[10.0, -5.0, 2.0]   |     8.2000     |     0.999725
```

### ğŸ“ Â¿Por quÃ© es Importante Sigmoid?

1. **No-linealidad**: Sin funciones de activaciÃ³n como sigmoid, una red neuronal serÃ­a solo una transformaciÃ³n lineal, sin importar cuÃ¡ntas capas tenga.

2. **Gradientes**: La funciÃ³n es diferenciable, permitiendo el algoritmo de backpropagation para entrenar la red.

3. **InterpretaciÃ³n**: Los valores entre 0 y 1 pueden interpretarse como probabilidades en problemas de clasificaciÃ³n binaria.

4. **Control de activaciÃ³n**: Determina cuÃ¡ndo y cuÃ¡nto se "activa" una neurona basÃ¡ndose en sus entradas.


### ğŸ“ Notas de Desarrollo

Este es el **DÃ­a 2** de una serie de aprendizaje progresivo sobre redes neuronales. El cÃ³digo estÃ¡ diseÃ±ado para ser educativo, con:
- Comentarios detallados en espaÃ±ol
- Salida explicativa paso a paso
- Ejemplos prÃ¡cticos con diferentes entradas
- Comparaciones directas entre enfoques lineales y no-lineales

---

*Implementado en Rust para practicar tanto conceptos de ML como programaciÃ³n de sistemas.*