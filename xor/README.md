# Red Neuronal Multicapa en Rust ğŸ§ 

Una implementaciÃ³n desde cero de una red neuronal multicapa en Rust, sin librerÃ­as externas, diseÃ±ada para aprender la funciÃ³n XOR mediante backpropagation.

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa una red neuronal feedforward de 3 capas que puede resolver problemas no linealmente separables como XOR. La implementaciÃ³n incluye:

- âœ… Forward propagation
- âœ… Backpropagation con gradient descent
- âœ… FunciÃ³n de activaciÃ³n sigmoide
- âœ… Entrenamiento completo con mÃºltiples Ã©pocas
- âœ… Sin dependencias externas (solo `std`)

## ğŸ—ï¸ Arquitectura de la Red

```
Entrada (2) â†’ Capa Oculta (3) â†’ Salida (1)
```

### Â¿Por quÃ© **2 entradas**?

Porque la red estÃ¡ resolviendo la **funciÃ³n lÃ³gica XOR**, que recibe **dos valores binarios** de entrada:

| Entrada A | Entrada B | XOR |
|-----------|-----------|-----|
| 0         | 0         | 0   |
| 0         | 1         | 1   |
| 1         | 0         | 1   |
| 1         | 1         | 0   |

â†’ Entonces tu input es un **vector de 2 valores** `[A, B]`  
Ejemplo: `[1.0, 0.0]` representa `1 XOR 0`.

### âœ… Â¿Por quÃ© **1 salida**?

Porque quieres que la red te diga el **resultado del XOR**:
* Si es `1`, la salida debe acercarse a `1.0`
* Si es `0`, la salida debe acercarse a `0.0`

Entonces, **una Ãºnica neurona de salida** basta.

### âœ… Â¿Por quÃ© **3 neuronas ocultas**?

La funciÃ³n XOR **no es linealmente separable**, asÃ­ que una red sin capa oculta no puede aprenderla.

* Con **1 o 2 neuronas ocultas**, a veces no es suficiente.
* Con **3 neuronas**, ya tienes el poder necesario para modelar la "curva" de XOR.

Con 3 se resuelve bien, con 4 aprendes mÃ¡s rÃ¡pido, con mÃ¡s de 5 para XOR es innecesario.

## âš™ï¸ ParÃ¡metros Clave

### ğŸ§  Â¿QuÃ© es el `learning_rate`?

El `learning_rate` o **tasa de aprendizaje** es **cuÃ¡nto cambia la red cada vez que aprende** de un error.

* Si es muy **pequeÃ±o** (`0.0001`), aprende lentamente.
* Si es muy **grande** (`10.0`), puede volverse inestable y no aprender nada.

Un valor tÃ­pico en redes pequeÃ±as como la tuya estÃ¡ entre `0.01` y `1.5`.  
En tu caso, usas `1.0`, que es bastante alto pero puede funcionar porque el problema es pequeÃ±o.

### ğŸ”¢ FunciÃ³n de Costo: Mean Squared Error (MSE)

```rust
total_error += 0.5 * error * error;
```

**Â¿Por quÃ© `0.5 * errorÂ²`?**
- El `0.5` hace que la derivada sea mÃ¡s limpia: cuando derivas, el 2 del exponente cancela el 0.5


### Salida Esperada

```
=== Red Neuronal Multicapa para XOR ===

Datos de entrenamiento (funciÃ³n XOR):
  [0.0, 0.0] â†’ [0.0]
  [0.0, 1.0] â†’ [1.0]
  [1.0, 0.0] â†’ [1.0]
  [1.0, 1.0] â†’ [0.0]

Ã‰poca 0: Error total = 1.234567
Ã‰poca 500: Error total = 0.123456
...
Ã‰poca 4500: Error total = 0.000123

=== Resultados despuÃ©s del entrenamiento ===

Pruebas finales:
Entrada: [0.0, 0.0] â†’ Salida: 0.0123, PredicciÃ³n: 0, Esperado: 0, âœ“
Entrada: [0.0, 1.0] â†’ Salida: 0.9876, PredicciÃ³n: 1, Esperado: 1, âœ“
Entrada: [1.0, 0.0] â†’ Salida: 0.9845, PredicciÃ³n: 1, Esperado: 1, âœ“
Entrada: [1.0, 1.0] â†’ Salida: 0.0234, PredicciÃ³n: 0, Esperado: 0, âœ“

ğŸ‰ Resultado: Â¡La red aprendiÃ³ XOR correctamente!
```

### Ajustar Learning Rate
```rust
let mut network = NeuralNetwork::new(2, 3, 1, 0.1);  // MÃ¡s conservador
let mut network = NeuralNetwork::new(2, 3, 1, 2.0);  // MÃ¡s agresivo
```


## âš ï¸ Escalabilidad: Â¿QuÃ© pasa con muchas capas?

Si quisieras **muchas capas ocultas (deep learning)**, aparecen nuevos desafÃ­os:

### Problemas con Redes Profundas
* **Vanishing Gradient**: El gradiente se vuelve muy pequeÃ±o en capas tempranas
* **Exploding Gradient**: El gradiente crece exponencialmente
* **Convergencia lenta**: MÃ¡s parÃ¡metros = mÃ¡s tiempo de entrenamiento



**Para XOR, 99 capas es como usar un tanque para abrir una lata.**

## ğŸ“š Conceptos Implementados

### Forward Propagation
1. **Entrada â†’ Capa Oculta**: `hidden[i] = sigmoid(sum(input[j] * weight[j][i]) + bias[i])`
2. **Capa Oculta â†’ Salida**: `output[i] = sigmoid(sum(hidden[j] * weight[j][i]) + bias[i])`

### Backpropagation
1. **Error en salida**: `(target - output) * output * (1 - output)`
2. **Error en capa oculta**: `suma_errores_salida * peso * hidden * (1 - hidden)`
3. **Actualizar pesos**: `peso_nuevo = peso_viejo + learning_rate * error * activaciÃ³n`

### FunciÃ³n Sigmoide y su Derivada
- **Sigmoide**: `Ïƒ(x) = 1/(1 + e^(-x))`
- **Derivada**: `Ïƒ'(x) = Ïƒ(x) * (1 - Ïƒ(x))`
*   [DÃ­a 2: Sigmoid](../sigmoid/README.md)

## ğŸ¯ PrÃ³ximos Pasos

1. **Diferentes funciones de activaciÃ³n**: ReLU, tanh, Leaky ReLU
2. **Optimizadores avanzados**: Momentum, Adam, RMSprop
3. **RegularizaciÃ³n**: L1, L2, Dropout
4. **Redes mÃ¡s profundas**: MÃºltiples capas ocultas
5. **Datasets reales**: MNIST, clasificaciÃ³n de imÃ¡genes

## ğŸ“ˆ MÃ©tricas de Rendimiento

- **Convergencia tÃ­pica**: 2000-5000 Ã©pocas para XOR
- **PrecisiÃ³n esperada**: >99% en datos de entrenamiento
- **Tiempo de ejecuciÃ³n**: <1 segundo en hardware moderno

## ğŸ› Debugging

Si la red no converge:
1. **Reduce el learning rate** (prueba 0.1 o 0.01)
2. **Aumenta las Ã©pocas** (10,000 en vez de 5,000)
3. **Cambia la inicializaciÃ³n** de pesos
4. **AÃ±ade mÃ¡s neuronas** ocultas (4 o 5)


---

*ImplementaciÃ³n creada como parte de un reto de 21 dÃ­as aprendiendo redes neuronales desde cero.*