# ğŸ§  Redes Neuronales: Tanh vs Sigmoid en Rust

## ğŸ“š Â¿QuÃ© hemos aprendido hoy?

Este proyecto demuestra de manera prÃ¡ctica las diferencias fundamentales entre las funciones de activaciÃ³n **Sigmoid** y **Tanh** en redes neuronales, implementadas en Rust con explicaciones detalladas paso a paso.

## ğŸ¯ Objetivos del Proyecto

- âœ… Implementar funciones de activaciÃ³n Sigmoid y Tanh
- âœ… Calcular sus derivadas para el proceso de backpropagation
- âœ… Comparar su comportamiento con ejemplos prÃ¡cticos
- âœ… Entender el problema del **gradiente desvaneciente**
- âœ… Demostrar por quÃ© el centrado en cero es importante

## ğŸ“Š Funciones de ActivaciÃ³n

### ğŸ”¸ Sigmoid
```
Ïƒ(x) = 1 / (1 + e^(-x))
```
- **Rango:** (0, 1)
- **CaracterÃ­sticas:** Siempre positiva, centrada en 0.5
- **Uso principal:** ClasificaciÃ³n binaria (capa de salida)

### ğŸ”¸ Tanh (Tangente HiperbÃ³lica)
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```
- **Rango:** (-1, 1)
- **CaracterÃ­sticas:** Centrada en 0, simÃ©trica
- **Uso principal:** Capas ocultas

## ğŸ§¨ Â¿QuÃ© es el *Gradiente Desvaneciente*?

Es cuando los **gradientes se hacen tan pequeÃ±os** (cercanos a cero) que **dejan de actualizar los pesos** de las capas anteriores. Es como decirle a la red: *"EstÃ¡s aprendiendo tan poquito que ya no vale la pena seguir."*

## ğŸ§® Â¿Por quÃ© pasa esto con `sigmoid` y `tanh`?

### ğŸ”¹ 1. **Por su forma: se aplastan en los extremos**

Mira las curvas de `sigmoid(x)` y `tanh(x)`:
* **Sigmoid(x)** va de 0 a 1
* **Tanh(x)** va de -1 a 1

Ambas tienen una forma en S, y se "aplanan" en los extremos. Esto significa que:
* Cuando `x` es muy grande o muy pequeÃ±o, los valores de la derivada (el gradiente) son muy cercanos a **0**
* Y si la derivada es casi 0, los **pesos casi no cambian** al entrenar

### ğŸ”¹ 2. **La derivada de sigmoid(x)**
```
d/dx sigmoid(x) = sigmoid(x)(1 - sigmoid(x))
```
ğŸ“‰ La derivada es mÃ¡xima en `x = 0` (valor 0.25), pero **se va acercando a 0 rÃ¡pidamente** cuando `x` se aleja de 0.

### ğŸ”¹ 3. **La derivada de tanh(x)**
```
d/dx tanh(x) = 1 - tanhÂ²(x)
```
ğŸ“‰ TambiÃ©n se aplana: cuando `tanh(x)` â‰ˆ Â±1, la derivada â‰ˆ 0.

## ğŸ“‰ Â¿QuÃ© pasa en una red neuronal profunda?

Cuando tienes muchas capas y usas `sigmoid` o `tanh` en cada una:

1. Los gradientes se multiplican por valores muy pequeÃ±os repetidamente
2. **Resultado:** los gradientes se "desvanecen" antes de llegar a las capas del inicio
3. **Consecuencia:** si no hay gradiente... **no hay aprendizaje** en las capas profundas

Esto es lo que se llama el **problema del gradiente desvaneciente**.

## ğŸ”„ Ejemplo Visual del Problema

```
Capa 5 (salida): gradiente = 0.3
Capa 4: 0.3 Ã— 0.2 = 0.06  â† Ya se estÃ¡ reduciendo
Capa 3: 0.06 Ã— 0.15 = 0.009  â† Muy pequeÃ±o
Capa 2: 0.009 Ã— 0.1 = 0.0009  â† Casi cero
Capa 1: 0.0009 Ã— 0.05 = 0.000045  â† Â¡Desvanecido!
```

## ğŸ’¡ ComparaciÃ³n PrÃ¡ctica

| Aspecto | Sigmoid | Tanh | Ganador |
|---------|---------|------|---------|
| **Rango** | (0, 1) | (-1, 1) | Tanh |
| **Centrado en cero** | âŒ No | âœ… SÃ­ | Tanh |
| **Gradientes fuertes** | âŒ DÃ©biles | âœ… MÃ¡s fuertes | Tanh |
| **ClasificaciÃ³n binaria** | âœ… Perfecto | âŒ No ideal | Sigmoid |
| **Capas ocultas** | âŒ ProblemÃ¡tico | âœ… Mejor | Tanh |
| **Velocidad de convergencia** | âŒ Lenta | âœ… MÃ¡s rÃ¡pida | Tanh |


## ğŸ“ˆ Resultados de las Demostraciones

### 1. **ComparaciÃ³n de Valores**
El programa muestra cÃ³mo cada funciÃ³n responde a diferentes entradas desde -3.0 hasta 3.0.

### 2. **Red Neuronal Simple**
Demuestra cÃ³mo las mismas entradas producen diferentes distribuciones:
- **Sigmoid:** valores entre 0.05 y 0.92
- **Tanh:** valores entre -0.96 y 0.92 (mÃ¡s distribuidos)

### 3. **DemostraciÃ³n de Centrado en Cero**
Explica por quÃ© las funciones centradas en cero facilitan el entrenamiento.

## ğŸ¯ Recomendaciones PrÃ¡cticas

### âœ… **Usa TANH cuando:**
- Trabajas con capas ocultas
- Necesitas gradientes mÃ¡s fuertes
- Quieres convergencia mÃ¡s rÃ¡pida
- Los datos estÃ¡n normalizados

### âœ… **Usa SIGMOID cuando:**
- Necesitas salida de probabilidad (0-1)
- ClasificaciÃ³n binaria en la capa de salida
- Interpretar como "porcentaje de activaciÃ³n"

### âš¡ **En la prÃ¡ctica moderna:**
- **ReLU** y sus variantes son mÃ¡s populares para capas ocultas
- **Sigmoid** sigue siendo Ãºtil para capas de salida
- **Tanh** aÃºn se usa en LSTMs y algunos casos especÃ­ficos

## ğŸ§ª Experimentos que puedes hacer

1. **Cambiar los valores de entrada:** Modifica el vector `test_values` para ver comportamientos extremos
2. **Implementar ReLU:** Agrega `relu(x) = max(0, x)` y compara
3. **Red mÃ¡s profunda:** Simula el gradiente desvaneciente con mÃ¡s capas
4. **VisualizaciÃ³n:** Usa un graficador para ver las curvas

## ğŸ“š Conceptos Clave Aprendidos

- **FunciÃ³n de activaciÃ³n:** Transforma la suma ponderada en una salida no lineal
- **Derivada:** Necesaria para calcular gradientes en backpropagation
- **Gradiente desvaneciente:** Problema donde los gradientes se vuelven muy pequeÃ±os
- **Centrado en cero:** CaracterÃ­stica que mejora el entrenamiento
- **SaturaciÃ³n:** Cuando la funciÃ³n se "aplana" y pierde sensibilidad

## ğŸ”— Recursos Adicionales

- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Understanding Activation Functions](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)
- [The Vanishing Gradient Problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)

