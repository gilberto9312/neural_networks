use std::f64::consts::E;

/// Estructura para representar una neurona simple
#[derive(Debug, Clone)]
 struct Neuron {
    pub synapses: f64,
    pub threshould: f64,
}

impl Neuron {
    pub fn new(value: f64) -> Self {
        Self {
            synapses: value,
            threshould: 0.0,
        }
    }
}

/// FunciÃ³n de activaciÃ³n Sigmoid
/// FÃ³rmula: Ïƒ(x) = 1 / (1 + e^(-x))
/// Rango: (0, 1)
 fn sigmoid(z: f64) -> f64 {
    println!("Calculando sigmoid({:.4}):", z);
    println!("  e^(-z) = e^(-{:.4}) = {:.6}", z, E.powf(-z));
    println!("  1 + e^(-z) = {:.6}", 1.0 + E.powf(-z));

    1.0 / (1.0 + E.powf(-z))
}

/// Derivada de la funciÃ³n Sigmoid
/// FÃ³rmula: Ïƒ'(x) = Ïƒ(x) * (1 - Ïƒ(x))
 fn sigmoid_derivative(x: f64) -> f64 {
    println!("Calculando sigmoid_derivative({:.4}):", x);
    
    let s = sigmoid(x);
    let one_minus_s = 1.0 - s;
    let derivative = s * one_minus_s;

    println!("  Ïƒ(x) = {:.6}", s);
    println!("  1 - Ïƒ(x) = {:.6}", one_minus_s);
    println!("  Ïƒ(x) * (1 - Ïƒ(x)) = {:.6}", derivative);

    derivative
}

/// FunciÃ³n de activaciÃ³n Tanh (Tangente HiperbÃ³lica)
/// FÃ³rmula: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
/// Rango: (-1, 1)
 fn tanh_activation(x: f64) -> f64 {
    let exp_x = E.powf(x);
    let exp_neg_x = E.powf(-x);

    println!("Calculando tanh({:.4}):", x);
    println!("  e^x = {:.6}", exp_x);
    println!("  e^(-x) = {:.6}", exp_neg_x);
    println!("  Numerador: e^x - e^(-x) = {:.6}", exp_x - exp_neg_x);
    println!("  Denominador: e^x + e^(-x) = {:.6}", exp_x + exp_neg_x);

    let tanh = (exp_x - exp_neg_x) / (exp_x + exp_neg_x);
    println!("  tanh(x) = {:.6}", tanh);

    tanh
}

/// Derivada de la funciÃ³n Tanh
/// FÃ³rmula: tanh'(x) = 1 - tanhÂ²(x)
 fn tanh_derivative(x: f64) -> f64 {
    println!("Calculando tanh_derivative({:.4}):", x);

    let t = tanh_activation(x);
    let t_squared = t * t;
    let derivative = 1.0 - t_squared;

    println!("  tanh(x) = {:.6}", t);
    println!("  tanhÂ²(x) = {:.6}", t_squared);
    println!("  1 - tanhÂ²(x) = {:.6}", derivative);

    derivative

}

/// FunciÃ³n para aplicar activaciÃ³n a una neurona
 fn apply_activation(neuron: &mut Neuron, activation_fn: fn(f64) -> f64) {
    neuron.threshould = activation_fn(neuron.synapses);
}

/// FunciÃ³n para demostrar las diferencias entre ambas activaciones
 fn compare_activations() {
    println!("=== COMPARACIÃ“N DETALLADA: TANH vs SIGMOID ===\n");
    
    // Valores de prueba
    let test_values = vec![-3.0, -1.0, -0.5, 0.0, 0.5, 1.0, 3.0];
    
    println!("| Input  | Sigmoid | Tanh   | Sig'   | Tanh'  |");
    println!("|--------|---------|--------|--------|--------|");
    
    for &x in &test_values {
        let sig_val = sigmoid(x);
        let tanh_val = tanh_activation(x);
        let sig_deriv = sigmoid_derivative(x);
        let tanh_deriv = tanh_derivative(x);
        
        println!("| {:6.1} | {:7.3} | {:6.3} | {:6.3} | {:6.3} |", 
                 x, sig_val, tanh_val, sig_deriv, tanh_deriv);
    }
    
    println!("\n=== ANÃLISIS DE CARACTERÃSTICAS ===");
    
    println!("\nğŸ“Š SIGMOID:");
    println!("â€¢ Rango de salida: (0, 1)");
    println!("â€¢ Siempre positiva");
    println!("â€¢ Centrada en 0.5");
    println!("â€¢ Problema del gradiente que desaparece para valores extremos");
    println!("â€¢ Ãštil para probabilidades y clasificaciÃ³n binaria");
    
    println!("\nğŸ“Š TANH:");
    println!("â€¢ Rango de salida: (-1, 1)");
    println!("â€¢ Centrada en 0 (media cercana a 0)");
    println!("â€¢ SimÃ©trica respecto al origen");
    println!("â€¢ Gradientes mÃ¡s fuertes que Sigmoid");
    println!("â€¢ Mejor para capas ocultas en redes neuronales");
    
    println!("\n=== VENTAJAS Y DESVENTAJAS ===");
    
    println!("\nâœ… VENTAJAS DE TANH:");
    println!("â€¢ Zero-centered: facilita el entrenamiento");
    println!("â€¢ Gradientes mÃ¡s fuertes en el rango medio");
    println!("â€¢ Convergencia mÃ¡s rÃ¡pida");
    
    println!("\nâŒ DESVENTAJAS DE TANH:");
    println!("â€¢ Sigue teniendo problema de gradiente desvaneciente");
    println!("â€¢ Computacionalmente mÃ¡s costosa que ReLU");
    
    println!("\nâœ… VENTAJAS DE SIGMOID:");
    println!("â€¢ InterpretaciÃ³n probabilÃ­stica clara");
    println!("â€¢ Suave y diferenciable");
    println!("â€¢ Ideal para la capa de salida en clasificaciÃ³n binaria");
    
    println!("\nâŒ DESVENTAJAS DE SIGMOID:");
    println!("â€¢ No centrada en cero");
    println!("â€¢ Problema severo del gradiente desvaneciente");
    println!("â€¢ SaturaciÃ³n en los extremos");
}

/// Ejemplo prÃ¡ctico: Red neuronal simple con ambas activaciones
 fn neural_network_example() {
    println!("\n=== EJEMPLO: RED NEURONAL SIMPLE ===\n");
    
    // Crear neuronas de entrada
    let inputs = vec![-2.0, -0.5, 0.0, 1.5, 2.5];
    
    println!("Entrada: {:?}", inputs);
    
    // Aplicar Sigmoid
    println!("\nğŸ”¸ Con activaciÃ³n SIGMOID:");
    let sigmoid_neurons: Vec<Neuron> = inputs.iter()
        .map(|&x| {
            let mut neuron = Neuron::new(x);
            apply_activation(&mut neuron, sigmoid);
            neuron
        })
        .collect();
    
    for (i, neuron) in sigmoid_neurons.iter().enumerate() {
        println!("Neurona {}: {:.3} â†’ {:.3}", 
                 i+1, neuron.synapses, neuron.threshould);
    }
    
    // Aplicar Tanh
    println!("\nğŸ”¸ Con activaciÃ³n TANH:");
    let tanh_neurons: Vec<Neuron> = inputs.iter()
        .map(|&x| {
            let mut neuron = Neuron::new(x);
            apply_activation(&mut neuron, tanh_activation);
            neuron
        })
        .collect();
    
    for (i, neuron) in tanh_neurons.iter().enumerate() {
        println!("Neurona {}: {:.3} â†’ {:.3}", 
                 i+1, neuron.synapses, neuron.threshould);
    }
    
    // Calcular estadÃ­sticas
    let sigmoid_sum: f64 = sigmoid_neurons.iter()
        .map(|n| n.threshould)
        .sum();
    let sigmoid_mean = sigmoid_sum / sigmoid_neurons.len() as f64;
    
    let tanh_sum: f64 = tanh_neurons.iter()
        .map(|n| n.threshould)
        .sum();
    let tanh_mean = tanh_sum / tanh_neurons.len() as f64;
    
    println!("\nğŸ“ˆ ESTADÃSTICAS:");
    println!("Media Sigmoid: {:.3}", sigmoid_mean);
    println!("Media Tanh: {:.3}", tanh_mean);
    println!("\nğŸ’¡ Observa cÃ³mo Tanh estÃ¡ mÃ¡s centrada en 0!");
}

/// DemostraciÃ³n de la importancia del centrado en cero
 fn zero_centered_demo() {
    println!("\n=== DEMOSTRACIÃ“N: IMPORTANCIA DEL CENTRADO EN CERO ===\n");
    
    let weights = vec![0.5, -0.3, 0.8, -0.2];
    let sigmoid_outputs = vec![0.8, 0.9, 0.7, 0.6]; // TÃ­picas salidas sigmoid
    let tanh_outputs = vec![0.3, 0.5, -0.2, -0.1]; // TÃ­picas salidas tanh
    
    println!("Pesos actuales: {:?}", weights);
    println!("Salidas Sigmoid: {:?}", sigmoid_outputs);
    println!("Salidas Tanh: {:?}", tanh_outputs);
    
    // Simular actualizaciÃ³n de gradientes
    let learning_rate = 0.1;
    let error_gradient = 1.0; // Gradiente simplificado
    
    println!("\nğŸ”„ ActualizaciÃ³n de pesos (simplificada):");
    println!("Con Sigmoid (siempre positivas):");
    for (i, &w) in weights.iter().enumerate() {
        let gradient = error_gradient * sigmoid_outputs[i];
        let new_weight = w - learning_rate * gradient;
        println!("  Peso[{}]: {:.3} â†’ {:.3} (Î” = {:.3})", 
                 i, w, new_weight, -learning_rate * gradient);
    }
    
    println!("\nCon Tanh (puede ser negativas):");
    for (i, &w) in weights.iter().enumerate() {
        let gradient = error_gradient * tanh_outputs[i];
        let new_weight = w - learning_rate * gradient;
        println!("  Peso[{}]: {:.3} â†’ {:.3} (Î” = {:.3})", 
                 i, w, new_weight, -learning_rate * gradient);
    }
    
    println!("\nğŸ’¡ Con Sigmoid, todos los gradientes tienen el mismo signo");
    println!("   â†’ Actualizaciones sesgadas");
    println!("ğŸ’¡ Con Tanh, los gradientes pueden tener signos diferentes");
    println!("   â†’ Actualizaciones mÃ¡s balanceadas");
}

fn main() {
    println!("ğŸ§  RETO DE REDES NEURONALES: TANH vs SIGMOID en Rust");
    println!("================================================\n");
    
    // Ejecutar todas las demostraciones
    compare_activations();
    neural_network_example();
    zero_centered_demo();
    
    println!("\n=== RESUMEN CLAVE ===");
    println!("ğŸ¯ Usa TANH para capas ocultas (mejor gradiente, centrada en 0)");
    println!("ğŸ¯ Usa SIGMOID para salida de clasificaciÃ³n binaria");
    println!("ğŸ¯ En prÃ¡ctica moderna, considera ReLU para capas ocultas");
    
    println!("\nâœ… Â¡Reto completado! Has aprendido las diferencias clave entre Tanh y Sigmoid.");
}