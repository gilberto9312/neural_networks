# D√≠a 5: Perceptr√≥n Entrenable desde Cero üß†

**Reto de 21 D√≠as - Redes Neuronales desde Cero**

## üéØ Objetivo del D√≠a

Implementar un **perceptr√≥n de una sola neurona** completamente desde cero en Rust, sin librer√≠as externas. El objetivo es entender los conceptos fundamentales del aprendizaje autom√°tico:

- C√≥mo una neurona artificial procesa informaci√≥n
- C√≥mo ajusta sus par√°metros para aprender
- C√≥mo resolver problemas de clasificaci√≥n binaria

## üßÆ Conceptos Aprendidos

### 1. Arquitectura del Perceptr√≥n
- **Entradas**: Valores que recibe la neurona (x‚ÇÅ, x‚ÇÇ, ...)
- **Pesos**: Par√°metros que se ajustan durante el entrenamiento (w‚ÇÅ, w‚ÇÇ, ...)
- **Bias**: Par√°metro que permite desplazar la funci√≥n de decisi√≥n
- **Funci√≥n de activaci√≥n**: Funci√≥n escal√≥n que produce la salida binaria

### 2. Matem√°ticas Fundamentales

**Forward Pass (Predicci√≥n):**
```
z = w‚ÇÅ√óx‚ÇÅ + w‚ÇÇ√óx‚ÇÇ + ... + bias
salida = step_function(z) = { 1 si z ‚â• 0
                            { 0 si z < 0
```

**Regla del Perceptr√≥n (Aprendizaje):**
```
error = salida_esperada - salida_obtenida
nuevo_peso = peso_actual + (tasa_aprendizaje √ó error √ó entrada)
nuevo_bias = bias_actual + (tasa_aprendizaje √ó error)
```

### 3. Proceso de Entrenamiento
1. Inicializar pesos con valores aleatorios peque√±os
2. Para cada ejemplo de entrenamiento:
   - Hacer predicci√≥n (forward pass)
   - Calcular error
   - Ajustar pesos y bias
3. Repetir hasta convergencia

## üöÄ C√≥mo Ejecutar

```bash
# Compilar y ejecutar
cargo run

# O directamente con rustc
rustc main.rs -o perceptron
./perceptron
```

## üìä Problema Resuelto: Compuerta AND

El perceptr√≥n aprende a implementar una compuerta l√≥gica AND:

| Entrada 1 | Entrada 2 | Salida |
|-----------|-----------|--------|
| 0         | 0         | 0      |
| 0         | 1         | 0      |
| 1         | 0         | 0      |
| 1         | 1         | 1      |

## üîß Estructura del C√≥digo

```rust
struct Perceptron {
    weights: Vec<f64>,      // Pesos de las conexiones
    bias: f64,              // T√©rmino de sesgo
    learning_rate: f64,     // Velocidad de aprendizaje
}
```

**M√©todos principales:**
- `new()`: Constructor con inicializaci√≥n aleatoria
- `predict()`: Realiza una predicci√≥n (forward pass)
- `train_step()`: Entrena con un solo ejemplo
- `train()`: Entrena con m√∫ltiples ejemplos durante varias √©pocas

## üß™ Experimentos Sugeridos

### 1. Cambiar la Tasa de Aprendizaje
```rust
let mut perceptron = Perceptron::new(2, 0.01); // M√°s lento
let mut perceptron = Perceptron::new(2, 0.5);  // M√°s r√°pido
let mut perceptron = Perceptron::new(2, 1.0);  // Muy r√°pido (¬øinestable?)
```

### 2. Probar con Compuerta OR
Descomenta la funci√≥n `create_training_data_or()` y √∫sala en lugar de los datos de AND.

### 3. Intentar con XOR (Spoiler: No Funcionar√°)
```rust
let training_data = vec![
    (vec![0.0, 0.0], 0.0), // 0 XOR 0 = 0
    (vec![0.0, 1.0], 1.0), // 0 XOR 1 = 1
    (vec![1.0, 0.0], 1.0), // 1 XOR 0 = 1
    (vec![1.0, 1.0], 0.0), // 1 XOR 1 = 0
];
```

**¬øPor qu√© no funciona XOR?** Porque no es linealmente separable. Necesitar√≠as m√∫ltiples neuronas (pr√≥ximos d√≠as).

### 4. Agregar Ruido a los Datos
Usa la funci√≥n `add_noise_to_data()` para ver c√≥mo se comporta el perceptr√≥n con datos imperfectos.

## üìà Salida Esperada

```
=== Entrenando Perceptr√≥n para compuerta AND ===

Estado inicial:
Pesos: [-0.123, 0.456]
Bias: 0.100

Datos de entrenamiento (compuerta AND):
[0.0, 0.0] -> 0
[0.0, 1.0] -> 0
[1.0, 0.0] -> 0
[1.0, 1.0] -> 1

√âpoca 0: Error total = 2.00
√âpoca 10: Error total = 0.00
¬°Perceptr√≥n entrenado perfectamente en 15 √©pocas!

=== Probando el perceptr√≥n entrenado ===
Entrada: [0.0, 0.0] -> Predicci√≥n: 0, Esperado: 0 ‚úì
Entrada: [0.0, 1.0] -> Predicci√≥n: 0, Esperado: 0 ‚úì
Entrada: [1.0, 0.0] -> Predicci√≥n: 0, Esperado: 0 ‚úì
Entrada: [1.0, 1.0] -> Predicci√≥n: 1, Esperado: 1 ‚úì
```

## ü§î Preguntas y Respuestas Fundamentales

### 1. ¬øPor qu√© inicializamos los pesos con valores aleatorios peque√±os?

**Valores aleatorios** son necesarios porque:
- Si todos los pesos empiezan iguales (ej: todos en 0), la neurona no puede "romper la simetr√≠a"
- Cada peso debe evolucionar de forma independiente para especializarse
- La aleatoriedad permite explorar diferentes soluciones iniciales

**Valores peque√±os** son importantes porque:
- Pesos grandes pueden saturar la funci√≥n de activaci√≥n desde el inicio
- Empezar cerca de cero permite ajustes graduales y suaves
- Evita que la neurona "se convenza" demasiado pronto de una decisi√≥n incorrecta

**Ejemplo**: Si inicializ√°ramos todos los pesos en 10.0, casi cualquier entrada dar√≠a una suma muy grande, haciendo que la neurona siempre prediga 1, dificultando el aprendizaje.

### 2. ¬øQu√© pasar√≠a si la tasa de aprendizaje fuera muy grande o muy peque√±a?

**Tasa muy peque√±a (ej: 0.001)**:
- ‚úÖ Convergencia estable y suave
- ‚ùå Aprendizaje extremadamente lento
- ‚ùå Puede quedarse atascado en m√≠nimos locales
- ‚ùå Necesita muchas m√°s √©pocas

**Tasa muy grande (ej: 2.0)**:
- ‚úÖ Aprendizaje r√°pido inicialmente
- ‚ùå Oscilaciones alrededor de la soluci√≥n √≥ptima
- ‚ùå Puede "saltar" sobre la soluci√≥n correcta
- ‚ùå Inestabilidad: los pesos pueden crecer descontroladamente

**Tasa equilibrada (ej: 0.1-0.5)**:
- ‚úÖ Balance entre velocidad y estabilidad
- ‚úÖ Convergencia en pocas √©pocas
- ‚úÖ Ajustes controlados

### 3. ¬øC√≥mo podr√≠amos visualizar la l√≠nea de decisi√≥n que crea el perceptr√≥n?

El perceptr√≥n crea una **l√≠nea de decisi√≥n** definida por la ecuaci√≥n:
```
w‚ÇÅ√óx‚ÇÅ + w‚ÇÇ√óx‚ÇÇ + bias = 0
```

**Opci√≥n 1: Visualizaci√≥n con caracteres**
```rust
fn visualizar_decision_2d(perceptron: &Perceptron) {
    println!("Mapa de decisi√≥n (0=Negro, 1=Blanco):");
    for y in 0..10 {
        for x in 0..10 {
            let input = vec![x as f64 / 9.0, y as f64 / 9.0];
            let prediction = perceptron.predict(&input);
            print!("{} ", if prediction == 1.0 { "‚¨ú" } else { "‚¨õ" });
        }
        println!();
    }
}
```

**Opci√≥n 2: Ecuaci√≥n matem√°tica de la l√≠nea**
```rust
fn mostrar_linea_decision(perceptron: &Perceptron) {
    let w1 = perceptron.weights[0];
    let w2 = perceptron.weights[1];
    let b = perceptron.bias;
    
    println!("L√≠nea de decisi√≥n: {:.3}√óx‚ÇÅ + {:.3}√óx‚ÇÇ + {:.3} = 0", w1, w2, b);
    
    // Para x‚ÇÅ = 0: x‚ÇÇ = -bias/w‚ÇÇ
    // Para x‚ÇÅ = 1: x‚ÇÇ = -(w‚ÇÅ + bias)/w‚ÇÇ
    if w2 != 0.0 {
        let y_cuando_x0 = -b / w2;
        let y_cuando_x1 = -(w1 + b) / w2;
        println!("L√≠nea pasa por: (0, {:.3}) y (1, {:.3})", y_cuando_x0, y_cuando_x1);
    }
}
```

### 4. ¬øQu√© otros problemas linealmente separables podr√≠as resolver?

**Compuertas l√≥gicas simples**:
- **OR**: Al menos una entrada debe ser 1
- **NOR**: Ninguna entrada debe ser 1 (NOT OR)  
- **NAND**: No ambas entradas pueden ser 1 (NOT AND)

**Problemas de clasificaci√≥n binaria del mundo real**:
```rust
// Ejemplo: Clasificar personas como "altas y pesadas"
let datos_altura_peso = vec![
    (vec![1.6, 60.0], 0.0), // Bajo, ligero -> 0
    (vec![1.9, 90.0], 1.0), // Alto, pesado -> 1  
    (vec![1.5, 50.0], 0.0), // Bajo, ligero -> 0
    (vec![1.8, 85.0], 1.0), // Alto, pesado -> 1
];

// Ejemplo: Decidir si aprobar un pr√©stamo
let datos_prestamo = vec![
    (vec![25000.0, 650.0], 0.0), // Salario bajo, mal cr√©dito -> No
    (vec![60000.0, 750.0], 1.0), // Salario alto, buen cr√©dito -> S√≠
    (vec![30000.0, 600.0], 0.0), // L√≠mites -> No
    (vec![80000.0, 720.0], 1.0), // Buenos n√∫meros -> S√≠
];

// Ejemplo: Detecci√≥n de spam simple
let datos_spam = vec![
    (vec![10.0, 2.0], 1.0),  // Muchos n√∫meros, pocas palabras -> Spam
    (vec![2.0, 15.0], 0.0),  // Pocos n√∫meros, muchas palabras -> No spam
    (vec![8.0, 3.0], 1.0),   // Muchos n√∫meros -> Spam
    (vec![1.0, 12.0], 0.0),  // Texto normal -> No spam
];
```

**Detecci√≥n de patrones geom√©tricos simples**:
- Detectar si un punto est√° arriba o abajo de una l√≠nea diagonal
- Clasificar temperaturas como "fr√≠o/calor" basado en temperatura y humedad
- Determinar si un estudiante pasar√° (horas de estudio + calificaci√≥n previa)

**‚ö†Ô∏è Importante - Lo que el perceptr√≥n NO puede resolver**:
- **XOR** (exclusivo o) - No es linealmente separable
- **Problemas no lineales** (c√≠rculos, espirales, formas curvas)
- **Clasificaci√≥n multi-clase** directa (necesita m√∫ltiples perceptrones)

La clave est√° en que debe existir una **l√≠nea recta** que pueda separar perfectamente las dos clases en el espacio de caracter√≠sticas.

## üéì Lo Que Hemos Logrado

- ‚úÖ Implementaci√≥n completa de un perceptr√≥n desde cero
- ‚úÖ Comprensi√≥n del proceso de aprendizaje supervisado
- ‚úÖ Experiencia pr√°ctica con forward pass y backpropagation b√°sica
- ‚úÖ Fundamentos matem√°ticos para redes neuronales m√°s complejas

---
**¬°D√≠a 5 completado! üéâ Solo quedan 16 d√≠as m√°s hacia el dominio de las redes neuronales PD: si el commit con el dia 4 es el mismo es porque el del dia 4 estuvo heavy.**